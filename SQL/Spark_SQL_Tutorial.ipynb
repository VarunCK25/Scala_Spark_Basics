{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://Varun-CK:4040\n",
       "SparkContext available as 'sc' (version = 2.3.0, master = local[*], app id = local-1579455488715)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-19 23:08:06 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-19 23:08:29 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@2a1822d5\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spark = SparkSession.builder().getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register temp table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dfTags: org.apache.spark.sql.DataFrame = [id: int, tag: string]\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dfTags = spark\n",
    "    .read\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .csv(\"..\\\\Resources\\\\question_tags_10K.csv\")\n",
    "    .toDF(\"id\", \"tag\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTags.createOrReplaceTempView(\"so_tags\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List all tables in Spark's catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-----------+---------+-----------+\n",
      "|   name|database|description|tableType|isTemporary|\n",
      "+-------+--------+-----------+---------+-----------+\n",
      "|so_tags|    null|       null|TEMPORARY|       true|\n",
      "+-------+--------+-----------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.catalog.listTables().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List all tables in Spark's catalog using Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----------+\n",
      "|database|tableName|isTemporary|\n",
      "+--------+---------+-----------+\n",
      "|        |  so_tags|       true|\n",
      "+--------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "// dfTags.select(\"id\", \"tag\").limit(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+\n",
      "| id|            tag|\n",
      "+---+---------------+\n",
      "|  1|           data|\n",
      "|  4|             c#|\n",
      "|  4|       winforms|\n",
      "|  4|type-conversion|\n",
      "|  4|        decimal|\n",
      "+---+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select id, tag from so_tags limit 10\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter by column value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "// dfTags.filter(\"tag == 'php'\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| id|tag|\n",
      "+---+---+\n",
      "| 23|php|\n",
      "| 42|php|\n",
      "| 85|php|\n",
      "|126|php|\n",
      "|146|php|\n",
      "+---+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from so_tags where tag = 'php'\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count number of rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "// println(s\"Number of php tags = ${ dfTags.filter(\"tag == 'php'\").count() }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|php_count|\n",
      "+---------+\n",
      "|      133|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select count(*) as php_count from so_tags where tag='php'\"\"\".stripMargin).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "// dfTags.filter(\"tag like 's%'\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+\n",
      "| id|          tag|\n",
      "+---+-------------+\n",
      "| 25|      sockets|\n",
      "| 36|          sql|\n",
      "| 36|   sql-server|\n",
      "| 40| structuremap|\n",
      "| 48|submit-button|\n",
      "+---+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select * from so_tags where tag like 's%'\"\"\".stripMargin).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL where with and clause"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "// dfTags.filter(\"tag like 's%'\").filter(\"id == 25 or id == 108\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "| id|    tag|\n",
      "+---+-------+\n",
      "| 25|sockets|\n",
      "|108|    svn|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select * from so_tags where tag like 's%' and (id = 25 or id = 108)\"\"\".stripMargin).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL IN clause"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "// dfTags.filter(\"id in (25, 108)\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+\n",
      "| id|      tag|\n",
      "+---+---------+\n",
      "| 25|      c++|\n",
      "| 25|        c|\n",
      "| 25|  sockets|\n",
      "| 25|mainframe|\n",
      "| 25|      zos|\n",
      "+---+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select * from so_tags where id in (25, 108)\"\"\".stripMargin).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL Group By"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "// dfTags.groupBy(\"tag\").count().show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|        tag|count|\n",
      "+-----------+-----+\n",
      "|type-safety|    4|\n",
      "|    jbutton|    1|\n",
      "|     iframe|    2|\n",
      "|  svn-hooks|    2|\n",
      "|  standards|    7|\n",
      "+-----------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select tag, count(*) as count from so_tags group by tag\"\"\".stripMargin).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL Group By with having clause"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "// dfTags.groupBy(\"tag\").count().filter(\"count > 5\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|          tag|count|\n",
      "+-------------+-----+\n",
      "|    standards|    7|\n",
      "|     keyboard|    8|\n",
      "|          rss|   12|\n",
      "|documentation|   15|\n",
      "|      session|    6|\n",
      "+-------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select tag, count(*) as count from so_tags group by tag having count > 5\"\"\".stripMargin).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL Order by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "// dfTags.groupBy(\"tag\").count().filter(\"count > 5\").orderBy(\"tag\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n",
      "|           tag|count|\n",
      "+--------------+-----+\n",
      "|          .net|  351|\n",
      "|      .net-2.0|   14|\n",
      "|      .net-3.5|   30|\n",
      "|         64bit|    7|\n",
      "|actionscript-3|   22|\n",
      "+--------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select tag, count(*) as count from so_tags group by tag having count > 5 order by tag\"\"\".stripMargin).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Typed dataframe, filter and temp table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dfQuestionsCSV: org.apache.spark.sql.DataFrame = [id: int, creation_date: timestamp ... 5 more fields]\n"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dfQuestionsCSV = spark\n",
    "    .read\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .option(\"dateFormat\",\"yyyy-MM-dd HH:mm:ss\")\n",
    "    .csv(\"..\\\\Resources\\\\questions_10K.csv\")\n",
    "    .toDF(\"id\", \"creation_date\", \"closed_date\", \"deletion_date\", \"score\", \"owner_userid\", \"answer_count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cast columns to data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dfQuestions: org.apache.spark.sql.DataFrame = [id: int, creation_date: timestamp ... 5 more fields]\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dfQuestions = dfQuestionsCSV.select(\n",
    "    dfQuestionsCSV.col(\"id\").cast(\"integer\"),\n",
    "    dfQuestionsCSV.col(\"creation_date\").cast(\"timestamp\"),\n",
    "    dfQuestionsCSV.col(\"closed_date\").cast(\"timestamp\"),\n",
    "    dfQuestionsCSV.col(\"deletion_date\").cast(\"date\"),\n",
    "    dfQuestionsCSV.col(\"score\").cast(\"integer\"),\n",
    "    dfQuestionsCSV.col(\"owner_userid\").cast(\"integer\"),\n",
    "    dfQuestionsCSV.col(\"answer_count\").cast(\"integer\")\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### filter dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dfQuestionsSubset: org.apache.spark.sql.DataFrame = [id: int, creation_date: timestamp ... 5 more fields]\n"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dfQuestionsSubset = dfQuestions.filter(\"score > 400 and score < 410\").toDF()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### register temp table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfQuestionsSubset.createOrReplaceTempView(\"so_questions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL Inner Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "// dfQuestionsSubset.join(dfTags, Seq(\"id\"), \"inner\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+---+-------------------+-------------------+-------------+-----+------------+------------+\n",
      "| id|      tag| id|      creation_date|        closed_date|deletion_date|score|owner_userid|answer_count|\n",
      "+---+---------+---+-------------------+-------------------+-------------+-----+------------+------------+\n",
      "|888|   xdebug|888|2008-08-04 04:48:21|2016-08-04 14:52:00|         null|  405|         131|          30|\n",
      "|888| phpstorm|888|2008-08-04 04:48:21|2016-08-04 14:52:00|         null|  405|         131|          30|\n",
      "|888|debugging|888|2008-08-04 04:48:21|2016-08-04 14:52:00|         null|  405|         131|          30|\n",
      "|888|  eclipse|888|2008-08-04 04:48:21|2016-08-04 14:52:00|         null|  405|         131|          30|\n",
      "|888|      php|888|2008-08-04 04:48:21|2016-08-04 14:52:00|         null|  405|         131|          30|\n",
      "+---+---------+---+-------------------+-------------------+-------------+-----+------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select t.*, q.* from so_questions q inner join so_tags t on t.id = q.id\"\"\".stripMargin).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL Left Outer Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "// dfQuestionsSubset.join(dfTags, Seq(\"id\"), \"left_outer\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+---+-------------------+-------------------+-------------+-----+------------+------------+\n",
      "| id|      tag| id|      creation_date|        closed_date|deletion_date|score|owner_userid|answer_count|\n",
      "+---+---------+---+-------------------+-------------------+-------------+-----+------------+------------+\n",
      "|888|   xdebug|888|2008-08-04 04:48:21|2016-08-04 14:52:00|         null|  405|         131|          30|\n",
      "|888| phpstorm|888|2008-08-04 04:48:21|2016-08-04 14:52:00|         null|  405|         131|          30|\n",
      "|888|debugging|888|2008-08-04 04:48:21|2016-08-04 14:52:00|         null|  405|         131|          30|\n",
      "|888|  eclipse|888|2008-08-04 04:48:21|2016-08-04 14:52:00|         null|  405|         131|          30|\n",
      "|888|      php|888|2008-08-04 04:48:21|2016-08-04 14:52:00|         null|  405|         131|          30|\n",
      "+---+---------+---+-------------------+-------------------+-------------+-----+------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select t.*, q.* from so_questions q left outer join so_tags t on t.id = q.id\"\"\".stripMargin).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL Right Outer Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "// dfTags.join(dfQuestionsSubset, Seq(\"id\"), \"right_outer\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+----+-------------+-----------+-------------+-----+------------+------------+\n",
      "| id|            tag|  id|creation_date|closed_date|deletion_date|score|owner_userid|answer_count|\n",
      "+---+---------------+----+-------------+-----------+-------------+-----+------------+------------+\n",
      "|  1|           data|null|         null|       null|         null| null|        null|        null|\n",
      "|  4|             c#|null|         null|       null|         null| null|        null|        null|\n",
      "|  4|       winforms|null|         null|       null|         null| null|        null|        null|\n",
      "|  4|type-conversion|null|         null|       null|         null| null|        null|        null|\n",
      "|  4|        decimal|null|         null|       null|         null| null|        null|        null|\n",
      "+---+---------------+----+-------------+-----------+-------------+-----+------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select t.*, q.* from so_questions q right outer join so_tags t on t.id = q.id\"\"\".stripMargin).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register `User Defined Function (UDF)` Function to prefix a String with so_ short for StackOverflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "prefixStackoverflow: (s: String)String\n"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def prefixStackoverflow(s: String): String = s\"so_$s\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Register User Defined Function (UDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res33: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(<function1>,StringType,Some(List(StringType)))\n"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.udf.register(\"prefix_so\", prefixStackoverflow _)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use udf prefix_so to augment each tag value with so_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+\n",
      "| id|UDF:prefix_so(tag)|\n",
      "+---+------------------+\n",
      "|  1|           so_data|\n",
      "|  4|             so_c#|\n",
      "|  4|       so_winforms|\n",
      "|  4|so_type-conversion|\n",
      "|  4|        so_decimal|\n",
      "+---+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select id, prefix_so(tag) from so_tags\"\"\".stripMargin).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL Distinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|        tag|\n",
      "+-----------+\n",
      "|type-safety|\n",
      "|    jbutton|\n",
      "|     iframe|\n",
      "|  svn-hooks|\n",
      "|  standards|\n",
      "+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select distinct tag from so_tags\"\"\".stripMargin).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Closing Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
