{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# @author: Varun CK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://Varun-CK:4040\n",
       "SparkContext available as 'sc' (version = 2.3.0, master = local[*], app id = local-1583833980777)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "df1: org.apache.spark.sql.DataFrame = [col1: int, col2: int]\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var df1 = Seq((1,2),(3,4)).toDF(\"col1\",\"col2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) show:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`show(n)`** displays the first n rows(default=20)\n",
    "\n",
    "**Example:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n",
      "|col1|col2|\n",
      "+----+----+\n",
      "|   1|   2|\n",
      "|   3|   4|\n",
      "+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) collect:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`collect()`** returns an array of all elements.\n",
    "\n",
    "**Example:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res1: Array[org.apache.spark.sql.Row] = Array([1,2], [3,4])\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) cache:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`cache()`** is a synonym for **`persist()`** with no storage level specified.\n",
    "\n",
    "**Example:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res2: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [col1: int, col2: int]\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) persist:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The persist method offers other options called **storage levels**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Storage levels lets us control:\n",
    " - Storage location (memory or disk)\n",
    " - Format in-memory\n",
    " - Partition replication\n",
    " \n",
    "**Example:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.storage.StorageLevel\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.storage.StorageLevel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-10 15:23:34 WARN  CacheManager:66 - Asked to cache already cached data.\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "res3: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [col1: int, col2: int]\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.persist(StorageLevel.MEMORY_AND_DISK)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the Storage Level Options:\n",
    "\n",
    " - StorageLevel.DISK_ONLY\n",
    " - StorageLevel.DISK_ONLY_2\n",
    " - StorageLevel.MEMORY_AND_DISK\n",
    " - StorageLevel.MEMORY_AND_DISK_2\n",
    " - StorageLevel.MEMORY_AND_DISK_SER\n",
    " - StorageLevel.MEMORY_AND_DISK_SER_2\n",
    " - StorageLevel.MEMORY_ONLY\n",
    " - StorageLevel.MEMORY_ONLY_2\n",
    " - StorageLevel.MEMORY_ONLY_SER\n",
    " - StorageLevel.MEMORY_ONLY_SER_2\n",
    " - StorageLevel.NONE\n",
    " - StorageLevel.OFF_HEAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MEMORY_ONLY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this storage level, RDD is stored as deserialized Java object in the JVM. If the size of RDD is greater than memory, It will not cache some partition and recompute them next time whenever needed. In this level the space used for storage is very high, the CPU computation time is low, the data is stored in-memory. It does not make use of the disk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MEMORY_AND_DISK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this level, RDD is stored as deserialized Java object in the JVM. When the size of RDD is greater than the size of memory, it stores the excess partition on the disk, and retrieve from disk whenever required. In this level the space used for storage is high, the CPU computation time is medium, it makes use of both in-memory and on disk storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MEMORY_ONLY_SER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This level of Spark store the RDD as serialized Java object (one-byte array per partition). It is more space efficient as compared to deserialized objects, especially when it uses fast serializer. But it increases the overhead on CPU. In this level the storage space is low, the CPU computation time is high and the data is stored in-memory. It does not make use of the disk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MEMORY_AND_DISK_SER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is similar to MEMORY_ONLY_SER, but it drops the partition that does not fits into memory to disk, rather than recomputing each time it is needed. In this storage level, The space used for storage is low, the CPU computation time is high, it makes use of both in-memory and on disk storage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DISK_ONLY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this storage level, RDD is stored only on disk. The space used for storage is low, the CPU computation time is high and it makes use of on disk storage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) reduce:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduce is a spark action that aggregates a data set (RDD) element using a function.\n",
    "That function takes two arguments and returns one.\n",
    "\n",
    "**Example:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[3] at parallelize at <console>:26\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var rdd = sc.parallelize(Seq(1, 2, 3)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res4: Int = 6\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.reduce((a, b) => a * b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) reduceByKey:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function passed to reduceByKey combines values from multiple keys. Function must be binary.\n",
    "\n",
    "**Note:**\n",
    "If the value is a string, we can use the groupByKey() to reduce it. \n",
    "\n",
    "**Example:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd: org.apache.spark.rdd.RDD[(Int, Int)] = ParallelCollectionRDD[4] at parallelize at <console>:26\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var rdd = sc.parallelize(Seq((1,2), (3,4), (3,6)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "output: org.apache.spark.rdd.RDD[(Int, Int)] = ShuffledRDD[5] at reduceByKey at <console>:28\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var output = rdd.reduceByKey((a,b) => (a+b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res5: Array[(Int, Int)] = Array((1,2), (3,10))\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7) GroupByKey:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`groupByKey`** groups all the values for each key in an RDD.\n",
    "**`groupByKey()`** is just to group our dataset based on a key.\n",
    "\n",
    "It will result in data shuffling when RDD is not already partitioned. \n",
    "\n",
    "**Example:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd2: org.apache.spark.rdd.RDD[(Int, Char)] = ParallelCollectionRDD[6] at parallelize at <console>:26\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var rdd2 = sc.parallelize(Seq((1,'a'), (2,'c'), (1,'b')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "output2: org.apache.spark.rdd.RDD[(Int, Iterable[Char])] = ShuffledRDD[7] at groupByKey at <console>:28\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var output2 = rdd2.groupByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res6: Array[(Int, Iterable[Char])] = Array((1,CompactBuffer(a, b)), (2,CompactBuffer(c)))\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output2.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Note:\n",
    "\n",
    " - Be careful using `groupByKey()` as it can cause a lot of data movement across the network and create large Iterables at workers.\n",
    "\n",
    "- Imagine you have an RDD where you have 1 million pairs that have the key 1. All of the values will have to fit in a single worker if you use group by key. So instead of a group by key, consider using reduced by key or a different key value transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8) aggregateByKey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9) combineByKey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refer below link: [combineByKey](http://www.hadoopexam.com/adi/index.php/spark-blog/90-how-combinebykey-works-in-spark-step-by-step-explaination)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10) count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`count()` returns the number of elements.\n",
    "\n",
    "**Example:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res7: Long = 2\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11) countByKey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`countByKey`** counts the value of RDD consisting of two components tuple for each distinct key. It actually counts the number of elements for each key and return the result to the master as lists of **(key, count)** pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd3: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[15] at parallelize at <console>:26\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd3 = sc.parallelize(Seq((\"Spark\",78),(\"Hive\",95),(\"spark\",15),(\"HBase\",25),(\"spark\",39),(\"BigData\",78),(\"spark\",49)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "output3: scala.collection.Map[String,Long] = Map(Hive -> 1, BigData -> 1, HBase -> 1, spark -> 3, Spark -> 1)\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var output3 = rdd3.countByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res8: scala.collection.Map[String,Long] = Map(Hive -> 1, BigData -> 1, HBase -> 1, spark -> 3, Spark -> 1)\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12) countByValue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`countByValue()`** returns the count of each unique value in an RDD as a local Map (as a Map to driver program) `(value, countofvalues)` pair.\n",
    "\n",
    "**`countByValue()`** is an action that returns the Map of each unique value with its count.\n",
    "\n",
    "**Example 1:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd4: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[18] at parallelize at <console>:26\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd4 = sc.parallelize{ Seq(10, 4, 3, 3) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res9: Array[Int] = Array(10, 4, 3, 3)\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd4.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res10: scala.collection.Map[Int,Long] = Map(4 -> 1, 10 -> 1, 3 -> 2)\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd4.countByValue()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 2:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd5: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[22] at parallelize at <console>:26\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd5 = sc.parallelize(Seq((\"HR\",5),(\"RD\",4),(\"ADMIN\",5),(\"SALES\",4),(\"SER\",6),(\"MAN\",8)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res11: scala.collection.Map[(String, Int),Long] = Map((HR,5) -> 1, (RD,4) -> 1, (SALES,4) -> 1, (ADMIN,5) -> 1, (MAN,8) -> 1, (SER,6) -> 1)\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd5.countByValue()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13) subtractByKey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very similar to `subtract`, but instead of supplying a function, the key-component of each pair will be automatically used as criterion for removing items from the first RDD.\n",
    "\n",
    "**Example:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "r1: org.apache.spark.rdd.RDD[(Int, Int)] = ParallelCollectionRDD[26] at parallelize at <console>:26\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val r1 = sc.parallelize(Seq((1,2), (3,4), (3,6)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "r2: org.apache.spark.rdd.RDD[(Int, Int)] = ParallelCollectionRDD[27] at parallelize at <console>:26\n"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val r2 = sc.parallelize(Seq((3,9)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "r3: org.apache.spark.rdd.RDD[(Int, Int)] = SubtractedRDD[28] at subtractByKey at <console>:30\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var r3 = r1.subtractByKey(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res12: Array[(Int, Int)] = Array((1,2), (3,4), (3,6))\n"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res13: Array[(Int, Int)] = Array((3,9))\n"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res14: Array[(Int, Int)] = Array((1,2))\n"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r3.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14) sort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RDD:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Return the RDD sorted by the given key function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataFrame:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To sort a dataframe\n",
    "\n",
    "**Example:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df2: org.apache.spark.sql.DataFrame = [col1: int]\n"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var df2 = Seq(3,8,6,10,1,5,9,4,7,2).toDF(\"col1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|col1|\n",
      "+----+\n",
      "|   3|\n",
      "|   8|\n",
      "|   6|\n",
      "|  10|\n",
      "|   1|\n",
      "|   5|\n",
      "|   9|\n",
      "|   4|\n",
      "|   7|\n",
      "|   2|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ascending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|col1|\n",
      "+----+\n",
      "|   1|\n",
      "|   2|\n",
      "|   3|\n",
      "|   4|\n",
      "|   5|\n",
      "|   6|\n",
      "|   7|\n",
      "|   8|\n",
      "|   9|\n",
      "|  10|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.sort(\"col1\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Descending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|col1|\n",
      "+----+\n",
      "|  10|\n",
      "|   9|\n",
      "|   8|\n",
      "|   7|\n",
      "|   6|\n",
      "|   5|\n",
      "|   4|\n",
      "|   3|\n",
      "|   2|\n",
      "|   1|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.sort($\"col1\".desc).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15) sortByKey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`sortByKey`** sorts key in `Pair RDD` in `ascending` or `descending` order.\n",
    "\n",
    "**Example:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd6: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[37] at parallelize at <console>:26\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd6 = sc.parallelize(Seq((\"India\",91),(\"USA\",1),(\"Brazil\",55),(\"Greece\",30),(\"China\",86),(\"Sweden\",46),(\"Turkey\",90),(\"Nepal\",977)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res18: Array[(String, Int)] = Array((India,91), (USA,1), (Brazil,55), (Greece,30), (China,86), (Sweden,46), (Turkey,90), (Nepal,977))\n"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd6.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ascending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "output_6_1: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[40] at sortByKey at <console>:28\n"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val output_6_1 = rdd6.sortByKey(true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res19: Array[(String, Int)] = Array((Brazil,55), (China,86), (Greece,30), (India,91), (Nepal,977), (Sweden,46), (Turkey,90), (USA,1))\n"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_6_1.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Descending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "output_6_2: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[43] at sortByKey at <console>:28\n"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val output_6_2 = rdd6.sortByKey(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res20: Array[(String, Int)] = Array((USA,1), (Turkey,90), (Sweden,46), (Nepal,977), (India,91), (Greece,30), (China,86), (Brazil,55))\n"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_6_2.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 16) fold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregate the elements of each partition, and then the results for all the partitions, using a given associative and commutative function and a neutral \"zero value\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 17) foldByKey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use foldByKey operation to aggregate values based on keys.\n",
    "\n",
    "**Example:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Reference-1](https://www.quora.com/What-is-the-difference-between-fold-and-foldByKey-in-Spark-What-are-some-real-use-cases)\n",
    "\n",
    "[Reference-2](http://timepasstechies.com/spark-pair-rdd-reducebykey-foldbykey-flatmap-aggregation-function-example-scala-java/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 18) foldleft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **`foldLeft`** method takes an associative binary operator function as parameter and will use it to collapse elements from the collection. The order for traversing the elements in the collection is from left to right and hence the name `foldLeft`. The foldLeft method allows you to also specify an initial value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `foldLeft` is fundamental in recursive function and will help you prevent stack-overflow exceptions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `foldLeft` method is a member of the `TraversableOnce` trait."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Reference](http://allaboutscala.com/tutorials/chapter-8-beginner-tutorial-using-scala-collection-functions/scala-foldleft-example/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Differences between fold, foldLeft and foldRight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Ref](https://coderwall.com/p/4l73-a/scala-fold-foldleft-and-foldright)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 19) top"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`top(n)`** returns the largest n elements using `natural ordering (RDD)`.\n",
    "\n",
    "**Example:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res21: Array[(Int, Char)] = Array((1,a), (2,c), (1,b))\n"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res22: Array[(Int, Char)] = Array((2,c), (1,b))\n"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.top(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20) first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`first`** returns the first element of the RDD.\n",
    "\n",
    "**Example:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res23: Array[(Int, Char)] = Array((1,a), (2,c), (1,b))\n"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res24: (Int, Char) = (1,a)\n"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 21) head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`head`** returns the first row of a dataframe.\n",
    "\n",
    "**Example:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n",
      "|col1|col2|\n",
      "+----+----+\n",
      "|   1|   2|\n",
      "|   3|   4|\n",
      "+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res26: org.apache.spark.sql.Row = [1,2]\n"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 22) head(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`head(n)`** returns the first n rows of a dataframe.\n",
    "\n",
    "**Example:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|col1|\n",
      "+----+\n",
      "|   3|\n",
      "|   8|\n",
      "|   6|\n",
      "|  10|\n",
      "|   1|\n",
      "+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res28: Array[org.apache.spark.sql.Row] = Array([3], [8], [6])\n"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 23) take(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`take(n)`** returns an array of the first n elements.\n",
    "\n",
    "**Example:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res29: Array[(Int, Char)] = Array((1,a), (2,c), (1,b))\n"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res30: Array[(Int, Char)] = Array((1,a), (2,c))\n"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returns the first n rows in the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|col1|\n",
      "+----+\n",
      "|   3|\n",
      "|   8|\n",
      "|   6|\n",
      "|  10|\n",
      "|   1|\n",
      "|   5|\n",
      "|   9|\n",
      "|   4|\n",
      "|   7|\n",
      "|   2|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res32: Array[org.apache.spark.sql.Row] = Array([3], [8], [6], [10])\n"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.take(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 24) Map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`map(function)`** creates a new RDD by performing a function on each record in the baseRDD.\n",
    "\n",
    "**Example:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "x: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[55] at parallelize at <console>:26\n"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val x = sc.parallelize(List(\"spark\", \"rdd\", \"example\",  \"sample\", \"example\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res33: Array[String] = Array(spark, rdd, example, sample, example)\n"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "y: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[56] at map at <console>:28\n"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val y = x.map(x => (x.toUpperCase()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res34: Array[String] = Array(SPARK, RDD, EXAMPLE, SAMPLE, EXAMPLE)\n"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n",
      "|col1|col2|\n",
      "+----+----+\n",
      "|   1|   2|\n",
      "|   3|   4|\n",
      "+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|value|\n",
      "+-----+\n",
      "|[1]_a|\n",
      "|[3]_a|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.select(\"col1\").map(x => (x+\"_a\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n",
      "|col1|col2|\n",
      "+----+----+\n",
      "|   2|   7|\n",
      "|   4|   9|\n",
      "+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.map(r => (r.getInt(0) + 1,r.getInt(1) + 5)).toDF(\"col1\",\"col2\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- col1: integer (nullable = false)\n",
      " |-- col2: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class DatasetExample\n"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case class DatasetExample(col1: Int, col2: Int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.Dataset\n"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ds1: org.apache.spark.sql.Dataset[DatasetExample] = [col1: int, col2: int]\n"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val ds1: Dataset[DatasetExample] = df1.as[DatasetExample]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|value|\n",
      "+-----+\n",
      "|[1]_a|\n",
      "|[3]_a|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ds1.select(\"col1\").map(x => (x+\"_a\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 25) FlatMap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returns a new RDD by first applying a function to all rows of the RDD, and then flattening the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 1:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "x1: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[77] at parallelize at <console>:27\n"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val x1 = sc.parallelize(List(\"spark\", \"Scala\", \"java helps\",  \"hello world\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res40: Array[String] = Array(spark, Scala, java helps, hello world)\n"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res41: Array[Array[String]] = Array(Array(spark), Array(rdd), Array(example), Array(sample), Array(example))\n"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.map(z => z.split(\",\")).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### flatMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res42: Array[String] = Array(spark, rdd, example, sample, example)\n"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.flatMap(z => z.split(\",\")).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 2:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd7: org.apache.spark.rdd.RDD[String] = ..\\Resources\\word_count_ex1.txt MapPartitionsRDD[81] at textFile at <console>:27\n"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd7 = sc.textFile(\"..\\\\Resources\\\\word_count_ex1.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res43: Array[String] = Array(the cat sat on the mat, the aardvark sat on the sofa)\n"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd7.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res44: Array[Array[String]] = Array(Array(the cat sat on the mat), Array(the aardvark sat on the sofa))\n"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd7.map(line => line.split(\",\")).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### flatMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res45: Array[String] = Array(the cat sat on the mat, the aardvark sat on the sofa)\n"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd7.flatMap(line => line.split(\",\")).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returns a new RDD by first applying a function to all rows of the DataFrame, and then flattening the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df3: org.apache.spark.sql.DataFrame = [col1: string, col2: string ... 2 more fields]\n"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df3 = Seq((\"A\",\"B\",\"x\",\"D\"),(\"A\",\"B\",\"y\",\"D\"),(\"A\",\"B\",\"z\",\"D\")).toDF(\"col1\",\"col2\",\"col3\",\"col4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+----+\n",
      "|col1|col2|col3|col4|\n",
      "+----+----+----+----+\n",
      "|   A|   B|   x|   D|\n",
      "|   A|   B|   y|   D|\n",
      "|   A|   B|   z|   D|\n",
      "+----+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ds3: org.apache.spark.sql.Dataset[(String, String, String, String)] = [col1: string, col2: string ... 2 more fields]\r\n",
       "res47: org.apache.spark.sql.DataFrame = [_1: string, _2: string ... 2 more fields]\n"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val ds3 = df3.as[(String, String, String, String)]\n",
    "ds3.flatMap { \n",
    "  case (x1, x2, x3, x4) => x3.split(\",\").map((x1, x2, _, x4))\n",
    "}.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+----+\n",
      "|col1|col2|col3|col4|\n",
      "+----+----+----+----+\n",
      "|   A|   B|   x|   D|\n",
      "|   A|   B|   y|   D|\n",
      "|   A|   B|   z|   D|\n",
      "+----+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ds3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 26) MapPartition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`mapPartitions`** returns a new RDD by applying a function to each partition of this DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark mapPartitions - Similar to map() transformation but in this case function runs separately on each partition (block) of RDD unlike map() where it was running on each element of partition. Hence mapPartitions are also useful when you are looking for performance gain (calls your function once/partition not once/element). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Suppose you have elements from 1 to 100 distributed among 10 partitions i.e. 10 elements/partition. map() transformation will call func 100 times to process these 100 elements but in case of mapPartitions(), func will be called once/partition i.e. 10 times. \n",
    "\n",
    "\n",
    " - Secondly, mapPartitions() holds the data in-memory i.e. it will store the result in memory until all the elements of the partition has been processed.\n",
    "\n",
    "\n",
    " - mapPartitions() will return the result only after it finishes processing of whole partition.\n",
    "\n",
    "\n",
    " - mapPartitions() requires an iterator input unlike map() transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is an Iterator?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An iterator is a way to access collection of elements one-by-one, its similar to collection of elements like List(), Array() etc in few ways but the difference is that iterator doesn't load the whole collection of elements in memory all together. Instead iterator loads elements one after another. In Scala you access these elements with hasNext and Next operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res49: Array[(Int, String)] = Array((1,Hello), (2,Hello), (3,Hello), (4,Hello), (5,Hello), (6,Hello), (7,Hello), (8,Hello), (9,Hello))\n"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize(1 to 9, 3).map(x=>(x, \"Hello\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res50: Int = 3\n"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize(1 to 9, 3).partitions.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### mapPartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res51: Array[String] = Array(Hello, Hello, Hello)\n"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize(1 to 9, 3).mapPartitions(x=>(Array(\"Hello\").iterator)).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### mapPartitions with iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res52: Array[Int] = Array(1, 4, 7)\n"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize(1 to 9, 3).mapPartitions(x=>(List(x.next).iterator)).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In first example, I have applied map() transformation on dataset distributed between 3 partitions so that you can see function is called 9 times.\n",
    "\n",
    "In second example, when we applied mapPartitions(), you will notice it ran 3 times i.e. for each partition once. We had to convert string \"Hello\" into iterator because mapPartitions() takes iterator as input.\n",
    "\n",
    "In third step, I tried to get the iterator next value to show you the element. Note that next is always increasing value, so you can't step back. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Reference](https://www.dataneb.com/forum/apache-spark/what-is-mappartitions-in-spark-example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 27) partition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **`partition`** in spark is an atomic chunk of data (logical division of data) stored on a node in the cluster.\n",
    "\n",
    "Partitions are basic units of parallelism in Apache Spark.\n",
    "\n",
    "RDDs in Apache Spark are collection of partitions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../Resources/Data+Partitioning+in+Spark.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd8: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[91] at parallelize at <console>:26\n"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var rdd8 = spark.sparkContext.parallelize(Array(\"jan\",\"feb\",\"mar\",\"april\",\"may\",\"jun\"),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res53: Int = 3\n"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd8.partitions.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 28) repartition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returns a new RDD/DataFrame partitioned by the given partitioning expressions into numPartitions. The resulting RDD/DataFrame is hash partitioned.\n",
    "\n",
    "This is the same operation as `\"DISTRIBUTE BY\"` in `SQL (Hive QL)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd8: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[95] at repartition at <console>:28\n"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd8 = rdd8.repartition(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res54: Int = 5\n"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd8.partitions.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 29) coalese"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **`coalesce`** method reduces the number of partitions in a RDD/DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd8: org.apache.spark.rdd.RDD[String] = CoalescedRDD[96] at coalesce at <console>:28\n"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd8 = rdd8.coalesce(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res55: Int = 2\n"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd8.partitions.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Differences between repartition and coalese:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **`repartition`** algorithm does a full shuffle of the data and creates equal sized partitions of data. \n",
    "\n",
    "The **`coalesce`** combines existing partitions to avoid a full shuffle.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Reference](https://medium.com/@mrpowers/managing-spark-partitions-with-coalesce-and-repartition-4050c57ad5c4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep in mind that repartitioning your data is a fairly expensive operation. Spark also has an optimized version of repartition() called coalesce() that allows avoiding data movement, but only if you are decreasing the number of RDD partitions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Coalese`** avoids a full shuffle. If it's known that the number is decreasing then the executor can safely keep data on the minimum number of partitions, only moving the data off the extra nodes, onto the nodes that we kept.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, it would go something like this:\n",
    "\n",
    "`\n",
    "Node 1 = 1,2,3\n",
    "Node 2 = 4,5,6\n",
    "Node 3 = 7,8,9\n",
    "Node 4 = 10,11,12\n",
    "`\n",
    "\n",
    "Then coalesce down to 2 partitions:\n",
    "\n",
    "`\n",
    "Node 1 = 1,2,3 + (10,11,12)\n",
    "Node 3 = 7,8,9 + (4,5,6)\n",
    "`\n",
    "\n",
    "*Notice that Node 1 and Node 3 did not require its original data to move.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 30) shared variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Shared variables`** are the variables that are required to be used by many functions & methods in parallel. Shared variables can be used in parallel operations.\n",
    "\t\n",
    "Spark segregates the job into the smallest possible operation, a closure, running on different nodes and each having a copy of all the variables of the Spark job.\n",
    "\n",
    "Any changes made to these variables doesn’t reflect in the driver program and hence to overcome this limitation Spark provides two special type of shared variables – **`Broadcast Variables`** and **`Accumulators`**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 31) Broadcast Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Used to cache a value in memory on all nodes. Here only one instance of this read-only variable is shared between all computations throughout the cluster.\n",
    "\t\n",
    "Spark sends the broadcast variable to each node concerned by the related task. After that, each node caches it locally in serialised form.\n",
    "\t\n",
    "Now before executing each of the planned tasks instead of getting values from the driver system retrieves them locally from the cache."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Broadcast variables are:\n",
    " - Immutable (Unchangeable)\n",
    " - Distributed i.e. broadcasted to the cluster\n",
    " - Fit in memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Syntax to create Broadcast variable:**\n",
    "\n",
    "`SparkContext.broadcast(Value)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Reference](https://supergloo.com/spark-scala/spark-broadcast-accumulator-examples-scala/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "broadcastVar: org.apache.spark.broadcast.Broadcast[Array[Int]] = Broadcast(59)\n"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val broadcastVar = sc.broadcast(Array(1, 2, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res56: Array[Int] = Array(1, 2, 3)\n"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "broadcastVar.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 32) Accumulators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As its name suggests **`Accumulators`** main role is to accumulate values. The accumulator is variables that are used to implement counters and sums. Spark provides accumulators of numeric type only.\n",
    "\t\n",
    "The user can create named or unnamed accumulators.\n",
    "\t\n",
    "Unlike Broadcast Variables, accumulators are writable. However, written values can be only read in driver program.\n",
    "\t\n",
    "It’s why accumulators work pretty well as data aggregators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Syntax to create accumulator:\n",
    "\n",
    "`SparkContext.accumulator(orgnlValue)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Reference](https://supergloo.com/spark-scala/spark-broadcast-accumulator-examples-scala/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "accum: org.apache.spark.Accumulator[Int] = 0\n"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val accum = sc.accumulator(0, \"Accumulator Example\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.parallelize(Array(1, 2, 3)).foreach(x => accum += x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res58: Int = 6\n"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accum.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 33) DAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`(Directed Acyclic Graph) DAG`** in Apache Spark is a set of Vertices and Edges, where vertices represent the RDDs and the edges represent the Operation to be applied on RDD.\n",
    "\n",
    "In Spark DAG, every edge directs from earlier to later in the sequence. On the calling of Action, the created DAG submits to DAG Scheduler which further splits the graph into the stages of the task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../Resources/DAG-Visualisation-01.jpg\" style=\"height:300px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../Resources/DAG-2.png\" style=\"height:500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 34) Physical execution plan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Apache Spark, a stage is a physical unit of execution. We can say, it is a step in a physical execution plan.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actions trigger the translation of the logical DAG into a physical execution plan. The Spark Catalyst query optimizer creates the physical execution plan for DataFrames, as shown in the diagram below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The physical plan identifies resources, such as memory partitions and compute tasks, that will execute the plan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../Resources/physical_execution_plan.png\" style=\"height:200px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Reference](https://mapr.com/blog/how-spark-runs-your-applications/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "file: String = ..\\Resources\\flights20170102.json\n"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var file = \"..\\\\Resources\\\\flights20170102.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class Flight\n"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case class Flight(_id: String, dofW: Long, carrier: String, origin: String, dest: String, crsdephour: Long, crsdeptime: Double,\n",
    "                  depdelay: Double,crsarrtime: Double, arrdelay: Double, crselapsedtime: Double, dist: Double)\n",
    "extends Serializable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df4: org.apache.spark.sql.Dataset[Flight] = [_id: string, arrdelay: double ... 10 more fields]\n"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df4 = spark.read.format(\"json\").option(\"inferSchema\", \"true\").load(file).as[Flight]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df5: org.apache.spark.sql.Dataset[Flight] = [_id: string, arrdelay: double ... 10 more fields]\n"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df5 = df4.filter($\"depdelay\" > 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res59: Array[Flight] = Array(Flight(ATL_BOS_2017-01-02_16_DL_1210,1,DL,ATL,BOS,16,1616.0,68.0,1852.0,49.0,156.0,946.0))\n"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df5.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Filter ('depdelay > 40)\n",
      "+- AnalysisBarrier\n",
      "      +- Relation[_id#271,arrdelay#272,carrier#273,crsarrtime#274,crsdephour#275L,crsdeptime#276,crselapsedtime#277,depdelay#278,dest#279,dist#280,dofW#281L,origin#282] json\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "_id: string, arrdelay: double, carrier: string, crsarrtime: double, crsdephour: bigint, crsdeptime: double, crselapsedtime: double, depdelay: double, dest: string, dist: double, dofW: bigint, origin: string\n",
      "Filter (depdelay#278 > cast(40 as double))\n",
      "+- Relation[_id#271,arrdelay#272,carrier#273,crsarrtime#274,crsdephour#275L,crsdeptime#276,crselapsedtime#277,depdelay#278,dest#279,dist#280,dofW#281L,origin#282] json\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Filter (isnotnull(depdelay#278) && (depdelay#278 > 40.0))\n",
      "+- Relation[_id#271,arrdelay#272,carrier#273,crsarrtime#274,crsdephour#275L,crsdeptime#276,crselapsedtime#277,depdelay#278,dest#279,dist#280,dofW#281L,origin#282] json\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Project [_id#271, arrdelay#272, carrier#273, crsarrtime#274, crsdephour#275L, crsdeptime#276, crselapsedtime#277, depdelay#278, dest#279, dist#280, dofW#281L, origin#282]\n",
      "+- *(1) Filter (isnotnull(depdelay#278) && (depdelay#278 > 40.0))\n",
      "   +- *(1) FileScan json [_id#271,arrdelay#272,carrier#273,crsarrtime#274,crsdephour#275L,crsdeptime#276,crselapsedtime#277,depdelay#278,dest#279,dist#280,dofW#281L,origin#282] Batched: false, Format: JSON, Location: InMemoryFileIndex[file:/F:/My_Practice/Scala_Spark_Basics/Resources/flights20170102.json], PartitionFilters: [], PushedFilters: [IsNotNull(depdelay), GreaterThan(depdelay,40.0)], ReadSchema: struct<_id:string,arrdelay:double,carrier:string,crsarrtime:double,crsdephour:bigint,crsdeptime:d...\n"
     ]
    }
   ],
   "source": [
    "df5.explain(true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../Resources/physical_execution_plan_result.png\" style=\"width:250px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 35) sparse vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **`sparse vector`** is represented by two parallel arrays: indices and values. Zero entries are not stored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "v: scala.collection.immutable.Vector[Int] = Vector(1, 0, 0, 0, 0, 0, 3)\n"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var v = Vector(1, 0, 0, 0, 0, 0, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sparse Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.linalg.Vectors\n"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.linalg.Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sv: org.apache.spark.ml.linalg.Vector = (7,[0,6],[1.0,3.0])\n"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var sv = Vectors.sparse(7, Array(0,6), Array(1,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 36) dense vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **`dense vector`** is backed by a double array representing its entry values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dv: org.apache.spark.ml.linalg.DenseVector = [1.0,0.0,0.0,0.0,0.0,0.0,3.0]\n"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var dv = sv.toDense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 37) pair RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - **`Pair RDDs`** are a special form o fRDD\n",
    " - Each element must be a key-value pair (a two-element tuple)\n",
    " - Keys and values can be anytype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "users: org.apache.spark.rdd.RDD[String] = ..\\Resources\\users.txt MapPartitionsRDD[107] at textFile at <console>:28\n"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val users = sc.textFile(\"..\\\\Resources\\\\users.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pairRdd: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[109] at map at <console>:30\n"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val pairRdd = users.map(line => line.split(\"\\t\")).map(fields => (fields(0), fields(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res61: Array[(String, String)] = Array((user001,Varun CK), (user022,Fred Flintstone), (user090,Bugs Bunny), (user111,Harry Potter))\n"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairRdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 38) filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`filter(function)`** creates a new RDD by including or excluding each record in the base RDD according to a Boolean function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd9: org.apache.spark.rdd.RDD[String] = ..\\Resources\\mydata.txt MapPartitionsRDD[111] at textFile at <console>:28\n"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var rdd9 = sc.textFile(\"..\\\\Resources\\\\mydata.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res62: Array[String] = Array(I've never seen a purple cow., I hope never to see one., But I can tell you, anyhow,, I'd rather see than one.)\n"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd9.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res63: Array[String] = Array(I've never seen a purple cow., I hope never to see one., I'd rather see than one.)\n"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd9.filter(line => line.startsWith(\"I\")).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`filter()`** function is used to filter the rows from DataFrame or Dataset based on the given condition or SQL expression, alternatively, we can also use **`where()`** operator instead of the filter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|col1|\n",
      "+----+\n",
      "|   3|\n",
      "|   8|\n",
      "|   6|\n",
      "|  10|\n",
      "|   1|\n",
      "|   5|\n",
      "|   9|\n",
      "|   4|\n",
      "|   7|\n",
      "|   2|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|col1|\n",
      "+----+\n",
      "|   8|\n",
      "|   6|\n",
      "|  10|\n",
      "|   9|\n",
      "|   7|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.filter($\"col1\" > 5).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|col1|\n",
      "+----+\n",
      "|   8|\n",
      "|   6|\n",
      "|  10|\n",
      "|   9|\n",
      "|   7|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.filter(\"col1 > 5\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|col1|\n",
      "+----+\n",
      "|   8|\n",
      "|   6|\n",
      "|  10|\n",
      "|   9|\n",
      "|   7|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.where(\"col1 > 5\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 39) sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`sample`** creates a new RDD with a sampling of elements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark sampling functions allows to take different samples following distributions or only take a couple of them.\n",
    "\n",
    "In Spark, there are two sampling operations, the transformation **sample** and the action **takeSample**.\n",
    "\n",
    "By using a transformation we can tell Spark to apply successive transformation on a sample of a given RDD. \n",
    "\n",
    "By using an action we retrieve a given sample and we can have it in local memory to be used by any other standard library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sample transformation takes up to three parameters SparkSampling:\n",
    "\n",
    " - First is weather the sampling is done with replacement or not.\n",
    " - Second is the sample size as a fraction. Finally we can optionally provide a random seed.\n",
    " - Finally we can optionally provide a random seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rawData: org.apache.spark.rdd.RDD[String] = ..\\Resources\\kddcup.data.gz MapPartitionsRDD[129] at textFile at <console>:28\n"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rawData = sc.textFile(\"..\\\\Resources\\\\kddcup.data.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sampledData: org.apache.spark.rdd.RDD[String] = PartitionwiseSampledRDD[130] at sample at <console>:30\n"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sampledData = rawData.sample(false, 0.1, 1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sampleDataSize: Long = 490191\n"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sampleDataSize = sampledData.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rawDataSize: Long = 4898431\n"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rawDataSize = rawData.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4898431 and after the sampling: 490191\n"
     ]
    }
   ],
   "source": [
    "println(rawDataSize + \" and after the sampling: \" + sampleDataSize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 40) Narrow transformation in RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Each partition in the child RDD depends on just one partition of the parent RDD\n",
    " - No shuffle required between executors\n",
    " - Can be collapsed into a single stage\n",
    " \n",
    "**Examples**: `map`, `filter`, and `union`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 41) Wide Transformation in RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Child partitions depend on multiple partitions in the parent RDD\n",
    " - Wide Transformationa defines a newstage\n",
    "\n",
    "**Examples**: `reduceByKey`, `join`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:**\n",
    "\n",
    "Wide transformations are also called `shuffle transformations` as they may or may not depend on a shuffle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 42) union"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returns an RDD containing data from both sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:**\n",
    "\n",
    " - Unlike the Mathematical Union, duplicates are not removed.\n",
    " - Also type should be same in both the RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd11: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[131] at parallelize at <console>:28\n"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd11 = sc.parallelize(List(\"lion\", \"tiger\", \"tiger\", \"peacock\", \"horse\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd12: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[132] at parallelize at <console>:28\n"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd12 = sc.parallelize(List(\"lion\", \"tiger\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res69: Array[String] = Array(lion, tiger, tiger, peacock, horse, lion, tiger)\n"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd11.union(rdd12).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 43) intersect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Returns elements that are common b/w both RDDs. \n",
    "- Also removes Duplicates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warning:**\n",
    "\n",
    "Involves shuffling & has worst performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res70: Array[String] = Array(lion, tiger, tiger, peacock, horse)\n"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd11.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res71: Array[String] = Array(lion, tiger)\n"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd12.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res72: Array[String] = Array(lion, tiger)\n"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd11.intersection(rdd12).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 44) distinct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returns distinct elements in the RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warning**:\n",
    "\n",
    "Involves shuffling of data over N/W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res73: Array[String] = Array(lion, tiger, tiger, peacock, horse)\n"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd11.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res74: Array[String] = Array(peacock, lion, horse, tiger)\n"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd11.distinct().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 45) subtract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Returns only elements that are present in the first RDD.\n",
    "- Preserves the duplicates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res75: Array[String] = Array(lion, tiger, tiger, peacock, horse)\n"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd11.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res76: Array[String] = Array(lion, tiger)\n"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd12.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res77: Array[String] = Array(peacock, horse)\n"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd11.subtract(rdd12).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 46) cartesian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provides cartesian product b/w 2 RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warning:**\n",
    "\n",
    "Is very expensive for large RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res78: Array[String] = Array(lion, tiger, tiger, peacock, horse)\n"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd11.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res79: Array[String] = Array(lion, tiger)\n"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd12.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res80: Array[(String, String)] = Array((lion,lion), (lion,tiger), (tiger,lion), (tiger,tiger), (tiger,lion), (tiger,tiger), (peacock,lion), (horse,lion), (peacock,tiger), (horse,tiger))\n"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd11.cartesian(rdd12).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joins (SQL and Core)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Joining data is an important part of many of our pipelines, and both Spark Core and SQL support the same fundamental types of joins. While joins are very common and powerful, they warrant special performance consideration as they may require large network transfers or even create datasets beyond our capability to handle.1 In core Spark it can be more important to think about the ordering of operations, since the DAG optimizer, unlike the SQL optimizer, isn’t able to re-order or push down filters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Core Spark Joins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will go over the RDD type joins. Joins in general are expensive since they require that corresponding keys from each RDD are located at the same partition so that they can be combined locally. If the RDDs do not have known partitioners, they will need to be shuffled so that both RDDs share a partitioner, and data with the same keys lives in the same partitions, as shown in Figure 4-1. If they have the same partitioner, the data may be colocated, as in Figure 4-3, so as to avoid network transfer. Regardless of whether the partitioners are the same, if one (or both) of the RDDs have a known partitioner only a narrow dependency is created, as in Figure 4-2. As with most key/value operations, the cost of the join increases with the number of keys and the distance the records have to travel in order to get to their correct partition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../Resources/RDD_Joins1.png\" style=\"height:200px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../Resources/RDD_Joins2.png\" style=\"height:230px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../Resources/RDD_Joins3.png\" style=\"height:250px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing an Execution Plan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to join data, Spark needs the data that is to be joined (i.e., the data based on each key) to live on the same partition. The default implementation of a join in Spark is a shuffled hash join. The shuffled hash join ensures that data on each partition will contain the same keys by partitioning the second dataset with the same default partitioner as the first, so that the keys with the same hash value from both datasets are in the same partition. While this approach always works, it can be more expensive than necessary because it requires a shuffle. The shuffle can be avoided if:\n",
    "\n",
    " - Both RDDs have a known partitioner.\n",
    " - One of the datasets is small enough to fit in memory, in which case we can do a broadcast hash join.\n",
    " \n",
    "Note that if the RDDs are colocated the network transfer can be avoided, along with the shuffle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speeding up joins by assigning a known partitioner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have to do an operation before the join that requires a shuffle, such as aggregateByKey or reduceByKey, you can prevent the shuffle by adding a hash partitioner with the same number of partitions as an explicit argument to the first operation before the join. You could make the example in the previous section even faster, by using the partitioner for the address data as an argument for the reduceByKey step, as in Example 4-4 and Figure 4-4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.rdd.RDD\r\n",
       "import org.apache.spark.HashPartitioner\n"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.HashPartitioner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "joinScoresWithAddress3: (scoreRDD: org.apache.spark.rdd.RDD[(Long, Double)], addressRDD: org.apache.spark.rdd.RDD[(Long, String)])org.apache.spark.rdd.RDD[(Long, (Double, String))]\n"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def joinScoresWithAddress3(scoreRDD: RDD[(Long, Double)],\n",
    "   addressRDD: RDD[(Long, String)]) : RDD[(Long, (Double, String))]= {\n",
    "    // If addressRDD has a known partitioner we should use that,\n",
    "    // otherwise it has a default hash parttioner, which we can reconstruct by\n",
    "    // getting the number of partitions.\n",
    "    val addressDataPartitioner = addressRDD.partitioner match {\n",
    "      case (Some(p)) => p\n",
    "      case (None) => new HashPartitioner(addressRDD.partitions.length)\n",
    "    }\n",
    "    val bestScoreData = scoreRDD.reduceByKey(addressDataPartitioner,\n",
    "      (x, y) => if(x > y) x else y)\n",
    "    bestScoreData.join(addressRDD)\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../Resources/RDD_Joins4.png\" style=\"height:250px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speeding up joins using a broadcast hash join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A broadcast hash join pushes one of the RDDs (the smaller one) to each of the worker nodes. Then it does a map-side combine with each partition of the larger RDD. If one of your RDDs can fit in memory or can be made to fit in memory it is always beneficial to do a broadcast hash join, since it doesn’t require a shuffle. Sometimes (but not always) Spark SQL will be smart enough to configure the broadcast join itself; in Spark SQL this is controlled with spark.sql.autoBroadcastJoinThreshold and spark.sql.broadcastTimeout. This is illustrated in Figure 4-5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../Resources/RDD_Joins5.png\" style=\"height:250px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark Core does not have an implementation of the broadcast hash join. Instead, we can manually implement a version of the broadcast hash join by collecting the smaller RDD to the driver as a map, then broadcasting the result, and using mapPartitions to combine the elements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partial manual broadcast hash join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes not all of our smaller RDD will fit into memory, but some keys are so overrepresented in the large dataset that you want to broadcast just the most common keys. This is especially useful if one key is so large that it can’t fit on a single partition. In this case you can use countByKeyApprox2 on the large RDD to get an approximate idea of which keys would most benefit from a broadcast. You then filter the smaller RDD for only these keys, collecting the result locally in a HashMap. Using sc.broadcast you can broadcast the HashMap so that each worker only has one copy and manually perform the join against the HashMap. Using the same HashMap you can then filter your large RDD down to not include the large number of duplicate keys and perform your standard join, unioning it with the result of your manual join. This approach is quite convoluted but may allow you to handle highly skewed data you couldn’t otherwise process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark SQL Joins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark SQL supports the same basic join types as core Spark, but the optimizer is able to do more of the heavy lifting for you—although you also give up some of your control. For example, Spark SQL can sometimes push down or reorder operations to make your joins more efficient. On the other hand, you don’t control the partitioner for DataFrames or Datasets, so you can’t manually avoid shuffles as you did with core Spark joins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame Joins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Joining data between DataFrames is one of the most common multi-DataFrame transformations. The standard SQL join types are all supported and can be specified as the joinType in df.join(otherDf, sqlCondition, joinType) when performing a join. As with joins between RDDs, joining with nonunique keys will result in the cross product (so if the left table has R1 and R2 with key1 and the right table has R3 and R5 with key1 you will get (R1, R3), (R1, R5), (R2, R3), (R2, R5)) in the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark’s supported join types are “inner,” “left_outer” (aliased as “outer”), “left_anti,” “right_outer,” “full_outer,” and “left_semi.”3 With the exception of “left_semi” these join types all join the two tables, but they behave differently when handling rows that do not have keys in both tables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Broadcast hash joins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Spark SQL you can see the type of join being performed by calling queryExecution.executedPlan. As with core Spark, if one of the tables is much smaller than the other you may want a broadcast hash join. You can hint to Spark SQL that a given DF should be broadcast for join by calling broadcast on the DataFrame before joining it (e.g., df1.join(broadcast(df2), \"key\")). Spark also automatically uses the spark.sql.conf.autoBroadcastJoinThreshold to determine if a table should be broadcast."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Joins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Joining Datasets is done with joinWith, and this behaves similarly to a regular relational join, except the result is a tuple of the different record types as shown in Example 4-11. This is somewhat more awkward to work with after the join, but also does make self joins, as shown in Example 4-12, much easier, as you don’t need to alias the columns first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example 4-11. Joining two Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`\n",
    "val result: Dataset[(RawPanda, CoffeeShop)] = pandas.joinWith(coffeeShops,\n",
    "      $\"zip\" === $\"zip\")\n",
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example 4-12. Self join a Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`\n",
    "val result: Dataset[(RawPanda, RawPanda)] = pandas.joinWith(pandas,\n",
    "      $\"zip\" === $\"zip\")\n",
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame Joins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../Resources/SQL_Joins.png\" style=\"height:500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, Spark offers:\n",
    "\n",
    "- Inner-Join,\n",
    "- Left-Join,\n",
    "- Right-Join,\n",
    "- Outer-Join\n",
    "- Cross-Join,\n",
    "- Left-Semi-Join,\n",
    "- Left-Anti-Semi-Join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "payment: org.apache.spark.sql.DataFrame = [paymentId: int, customerId: int ... 1 more field]\n"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val payment = sc.parallelize(Seq(\n",
    "  (1, 101,2500), (2,102,1110), (3,103,500), (4 ,104,400), (5 ,105, 150), (6 ,106, 450)\n",
    ")).toDF(\"paymentId\", \"customerId\",\"amount\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+------+\n",
      "|paymentId|customerId|amount|\n",
      "+---------+----------+------+\n",
      "|        1|       101|  2500|\n",
      "|        2|       102|  1110|\n",
      "|        3|       103|   500|\n",
      "|        4|       104|   400|\n",
      "|        5|       105|   150|\n",
      "|        6|       106|   450|\n",
      "+---------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "payment.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "customer: org.apache.spark.sql.DataFrame = [customerId: int, name: string]\n"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val customer = sc.parallelize(Seq((101,\"Jon\") , (102,\"Aron\") ,(103,\"Sam\"))).toDF(\"customerId\", \"name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+\n",
      "|customerId|name|\n",
      "+----------+----+\n",
      "|       101| Jon|\n",
      "|       102|Aron|\n",
      "|       103| Sam|\n",
      "+----------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customer.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 47) Inner-Join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - This is the default join in Spark. Inner join basically removes all the things that are not common in both the tables. \n",
    " - It returns back all the data that has a match on the join condition from both sides of the table. \n",
    " - It is basically an intersection of sets on the join column if you visualize in terms of a Venn diagram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "innerJoinDf: org.apache.spark.sql.DataFrame = [customerId: int, name: string ... 2 more fields]\n"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val innerJoinDf = customer.join(payment,\"customerId\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+---------+------+\n",
      "|customerId|name|paymentId|amount|\n",
      "+----------+----+---------+------+\n",
      "|       101| Jon|        1|  2500|\n",
      "|       103| Sam|        3|   500|\n",
      "|       102|Aron|        2|  1110|\n",
      "+----------+----+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "innerJoinDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So in the example, only customerId 101,102,103 have entries in both the tables hence inner join returns only those.\n",
    "\n",
    "Note that it can have duplicate ids (which is generally not a case).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some other extra things that spark gives us out of the box:\n",
    "\n",
    "- Spark automatically removes duplicated “customerId” column, so column names are unique(When we use the above-mentioned syntax, more on this later).\n",
    "- And you don’t have to prefix the table name to address them in the join clause which gets really wasteful sometimes.\n",
    "- We did not specify that we wanted to do an “inner-join”, by default spark performs an inner-join if no join type is given."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Some gotchas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "34: error: overloaded method value join with alternatives:\r",
     "output_type": "error",
     "traceback": [
      "<console>:34: error: overloaded method value join with alternatives:\r",
      "  (right: org.apache.spark.sql.Dataset[_],joinExprs: org.apache.spark.sql.Column,joinType: String)org.apache.spark.sql.DataFrame <and>\r",
      "  (right: org.apache.spark.sql.Dataset[_],usingColumns: Seq[String],joinType: String)org.apache.spark.sql.DataFrame\r",
      " cannot be applied to (org.apache.spark.sql.DataFrame, String, String)\r",
      "       val innerJoinDf = customer.join(payment,\"customerId\", \"inner\")\r",
      "                                  ^",
      ""
     ]
    }
   ],
   "source": [
    "val innerJoinDf = customer.join(payment,\"customerId\", \"inner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This wouldn’t work and error out as shown above. Either you should skip the join type or the column name should be wrapped into scala `Seq` if have a join type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "innerJoinDf1: org.apache.spark.sql.DataFrame = [customerId: int, name: string ... 2 more fields]\n"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val innerJoinDf1 = customer.join(payment,Seq(\"customerId\"), \"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+---------+------+\n",
      "|customerId|name|paymentId|amount|\n",
      "+----------+----+---------+------+\n",
      "|       101| Jon|        1|  2500|\n",
      "|       103| Sam|        3|   500|\n",
      "|       102|Aron|        2|  1110|\n",
      "+----------+----+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "innerJoinDf1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 48) Left Join:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In a left join, all the rows from the left table are returned irrespective of whether there is a match in the right side table.\n",
    "- If a matching id is found in the right table is found, it is returned or else a null is appended. \n",
    "- We use Left Join when nulls matter, make sense of data when there were no sales or something like that. Say we need all the days when there was no payment activity in the Rental Store."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`\n",
    "object JoinType {\n",
    "  def apply(typ: String): JoinType = typ.toLowerCase(Locale.ROOT).replace(\"_\", \"\") match {\n",
    "    case \"inner\" => Inner\n",
    "    case \"outer\" | \"full\" | \"fullouter\" => FullOuter\n",
    "    case \"leftouter\" | \"left\" => LeftOuter\n",
    "    case \"rightouter\" | \"right\" => RightOuter\n",
    "    case \"leftsemi\" => LeftSemi\n",
    "    case \"leftanti\" => LeftAnti\n",
    "    case \"cross\" => Cross\n",
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, as you can see this is from the spark source code that the Left and left outer join are the same. It is just an alias in Spark code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "leftJoinDf: org.apache.spark.sql.DataFrame = [customerId: int, paymentId: int ... 2 more fields]\n"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val leftJoinDf = payment.join(customer,Seq(\"customerId\"), \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+------+----+\n",
      "|customerId|paymentId|amount|name|\n",
      "+----------+---------+------+----+\n",
      "|       101|        1|  2500| Jon|\n",
      "|       103|        3|   500| Sam|\n",
      "|       102|        2|  1110|Aron|\n",
      "|       105|        5|   150|null|\n",
      "|       106|        6|   450|null|\n",
      "|       104|        4|   400|null|\n",
      "+----------+---------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "leftJoinDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+------+----+\n",
      "|customerId|paymentId|amount|name|\n",
      "+----------+---------+------+----+\n",
      "|       101|        1|  2500| Jon|\n",
      "|       103|        3|   500| Sam|\n",
      "|       102|        2|  1110|Aron|\n",
      "|       105|        5|   150|null|\n",
      "|       106|        6|   450|null|\n",
      "|       104|        4|   400|null|\n",
      "+----------+---------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "payment.join(customer,Seq(\"customerId\"), \"left_outer\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are joining on `customerId` , and as you can see the resulting dataframe has all the entries for the rows in payment table. It is populated with customer data wherever a matching record is found in the right side Customer table else `nulls` are returned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 49) Right Join:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is similar to Left join. In Right join, all the rows from the Right table are returned irrespective of whether there is a match in the left side table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rightJoinDf: org.apache.spark.sql.DataFrame = [customerId: int, paymentId: int ... 2 more fields]\n"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rightJoinDf = payment.join(customer,Seq(\"customerId\"), \"right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+------+----+\n",
      "|customerId|paymentId|amount|name|\n",
      "+----------+---------+------+----+\n",
      "|       101|        1|  2500| Jon|\n",
      "|       103|        3|   500| Sam|\n",
      "|       102|        2|  1110|Aron|\n",
      "+----------+---------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rightJoinDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+------+----+\n",
      "|customerId|paymentId|amount|name|\n",
      "+----------+---------+------+----+\n",
      "|       101|        1|  2500| Jon|\n",
      "|       103|        3|   500| Sam|\n",
      "|       102|        2|  1110|Aron|\n",
      "+----------+---------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "payment.join(customer,Seq(\"customerId\"), \"right_outer\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the right side table is the customer, hence all the data from the customer table is returned back. Since there is no matching data on the left side payment table, no nulls are appended as a part of the output.\n",
    "\n",
    "Also, the right join and Right Outer join yield the same output. Theoretically speaking all the things that could be achieved from the right join can be achieved by using left join but there can be few scenarios where right-join might come in handy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 50) Outer Join:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We use full outer joins to keep records from both the tables along with the associated null values in the respective left/right tables. \n",
    "- It is kind of rare but generally used exception reports or situations when you would require data from both the tables. \n",
    "- For example, you want to find a department which doesn’t have an employee and also find an employee who doesn’t have a department and also find a department which has an employee."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>\n",
    "val fullJoinDf = employees.join(departments, Seq(\"departmentID\"), \"outer\")\n",
    "fullJoinDf: org.apache.spark.sql.DataFrame = [departmentId: int, name: string ... 1 more field]\n",
    "\n",
    "fullJoinDf.show\n",
    "+------------+------+--------------+\n",
    "|departmentId|  name|departmentName|\n",
    "+------------+------+--------------+\n",
    "|          31|   Amy|            IT|\n",
    "|          34|   Rob|       Medical|\n",
    "|          34| Billy|       Medical|\n",
    "|          27|Larson|          null|\n",
    "|          35|  null|         Sales|\n",
    "|          33| Bobby|           Law|\n",
    "|          33| Richy|           Law|\n",
    "+------------+------+--------------+\n",
    "</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, you can see that there is no employee in the sales dept and also Larson is not associated with any department."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 2:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fullJoinDf: org.apache.spark.sql.DataFrame = [customerId: int, paymentId: int ... 2 more fields]\n"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val fullJoinDf = payment.join(customer,Seq(\"customerId\"), \"outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+------+----+\n",
      "|customerId|paymentId|amount|name|\n",
      "+----------+---------+------+----+\n",
      "|       101|        1|  2500| Jon|\n",
      "|       103|        3|   500| Sam|\n",
      "|       102|        2|  1110|Aron|\n",
      "|       105|        5|   150|null|\n",
      "|       106|        6|   450|null|\n",
      "|       104|        4|   400|null|\n",
      "+----------+---------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fullJoinDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 51) Cross Join:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the saying goes, the cross product of big data and big data is an out-of-memory exception(From Holden’s High-Performance Spark). \n",
    "\n",
    "Even before you start to read about it, try avoiding this with big tables in production. Unless it is the only way to do. Cross join basically computes a cartesian product of 2 tables. \n",
    "\n",
    "Say you have m rows in 1 table n rows in another, this would give (m*n) rows. So imagine a small table 10,000 customer table joined with a products table of 1000 records would give an exploding 10,000,000 records!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "crossJoinDf: org.apache.spark.sql.DataFrame = [customerId: int, name: string ... 3 more fields]\n"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val crossJoinDf = customer.crossJoin(payment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+---------+----------+------+\n",
      "|customerId|name|paymentId|customerId|amount|\n",
      "+----------+----+---------+----------+------+\n",
      "|       101| Jon|        1|       101|  2500|\n",
      "|       101| Jon|        2|       102|  1110|\n",
      "|       101| Jon|        3|       103|   500|\n",
      "|       101| Jon|        4|       104|   400|\n",
      "|       101| Jon|        5|       105|   150|\n",
      "|       101| Jon|        6|       106|   450|\n",
      "|       102|Aron|        1|       101|  2500|\n",
      "|       102|Aron|        2|       102|  1110|\n",
      "|       102|Aron|        3|       103|   500|\n",
      "|       102|Aron|        4|       104|   400|\n",
      "|       102|Aron|        5|       105|   150|\n",
      "|       102|Aron|        6|       106|   450|\n",
      "|       103| Sam|        1|       101|  2500|\n",
      "|       103| Sam|        2|       102|  1110|\n",
      "|       103| Sam|        3|       103|   500|\n",
      "|       103| Sam|        4|       104|   400|\n",
      "|       103| Sam|        5|       105|   150|\n",
      "|       103| Sam|        6|       106|   450|\n",
      "+----------+----+---------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "crossJoinDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark is kind of restricting the users to accidentally trigger a cartesian join when no join condition was specified. Prior Spark 2.1, `customer.join(payment)` would trigger a cross join. But now Spark throws an AnalysisException when the user forgets to give a condition on the joins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res91: org.apache.spark.sql.DataFrame = [customerId: int, name: string ... 3 more fields]\n"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customer.join(payment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "ename": "org.apache.spark.sql.AnalysisException",
     "evalue": " Detected cartesian product for INNER join between logical plans\r",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.sql.AnalysisException: Detected cartesian product for INNER join between logical plans\r",
      "Project [_1#375 AS customerId#378, _2#376 AS name#379]\r",
      "+- SerializeFromObject [assertnotnull(input[0, scala.Tuple2, true])._1 AS _1#375, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, assertnotnull(input[0, scala.Tuple2, true])._2, true, false) AS _2#376]\r",
      "   +- ExternalRDD [obj#374]\r",
      "and\r",
      "Project [_1#351 AS paymentId#355, _2#352 AS customerId#356, _3#353 AS amount#357]\r",
      "+- SerializeFromObject [assertnotnull(input[0, scala.Tuple3, true])._1 AS _1#351, assertnotnull(input[0, scala.Tuple3, true])._2 AS _2#352, assertnotnull(input[0, scala.Tuple3, true])._3 AS _3#353]\r",
      "   +- ExternalRDD [obj#350]\r",
      "Join condition is missing or trivial.\r",
      "Use the CROSS JOIN syntax to allow cartesian products between these relations.;\r",
      "  at org.apache.spark.sql.catalyst.optimizer.CheckCartesianProducts$$anonfun$apply$21.applyOrElse(Optimizer.scala:1124)\r",
      "  at org.apache.spark.sql.catalyst.optimizer.CheckCartesianProducts$$anonfun$apply$21.applyOrElse(Optimizer.scala:1121)\r",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$2.apply(TreeNode.scala:267)\r",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$2.apply(TreeNode.scala:267)\r",
      "  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\r",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:266)\r",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)\r",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)\r",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)\r",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\r",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)\r",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:272)\r",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)\r",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)\r",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)\r",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\r",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)\r",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:272)\r",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)\r",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)\r",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)\r",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\r",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)\r",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:272)\r",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:256)\r",
      "  at org.apache.spark.sql.catalyst.optimizer.CheckCartesianProducts$.apply(Optimizer.scala:1121)\r",
      "  at org.apache.spark.sql.catalyst.optimizer.CheckCartesianProducts$.apply(Optimizer.scala:1103)\r",
      "  at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:87)\r",
      "  at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:84)\r",
      "  at scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:57)\r",
      "  at scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:66)\r",
      "  at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:35)\r",
      "  at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:84)\r",
      "  at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:76)\r",
      "  at scala.collection.immutable.List.foreach(List.scala:381)\r",
      "  at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:76)\r",
      "  at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:66)\r",
      "  at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:66)\r",
      "  at org.apache.spark.sql.execution.QueryExecution.sparkPlan$lzycompute(QueryExecution.scala:72)\r",
      "  at org.apache.spark.sql.execution.QueryExecution.sparkPlan(QueryExecution.scala:68)\r",
      "  at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:77)\r",
      "  at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:77)\r",
      "  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3248)\r",
      "  at org.apache.spark.sql.Dataset.head(Dataset.scala:2484)\r",
      "  at org.apache.spark.sql.Dataset.take(Dataset.scala:2698)\r",
      "  at org.apache.spark.sql.Dataset.showString(Dataset.scala:254)\r",
      "  at org.apache.spark.sql.Dataset.show(Dataset.scala:723)\r",
      "  at org.apache.spark.sql.Dataset.show(Dataset.scala:682)\r",
      "  at org.apache.spark.sql.Dataset.show(Dataset.scala:691)\r",
      "  ... 36 elided",
      ""
     ]
    }
   ],
   "source": [
    "customer.join(payment).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, to bypass this AnalysisException we have to either set the `spark.sql.crossJoin.enabled=true` in our Spark session builder object or set it for `spark-shell :spark-shell — conf spark.sql.crossJoin.enabled=true`. We can verify if this property is set by checking,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From Spark 2.1, a dedicated function for Cross join has been added to support Cartesian joins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "crossJoin: org.apache.spark.sql.DataFrame = [customerId: int, name: string ... 3 more fields]\n"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val crossJoin = customer.crossJoin(payment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+---------+----------+------+\n",
      "|customerId|name|paymentId|customerId|amount|\n",
      "+----------+----+---------+----------+------+\n",
      "|       101| Jon|        1|       101|  2500|\n",
      "|       101| Jon|        2|       102|  1110|\n",
      "|       101| Jon|        3|       103|   500|\n",
      "|       101| Jon|        4|       104|   400|\n",
      "|       101| Jon|        5|       105|   150|\n",
      "|       101| Jon|        6|       106|   450|\n",
      "|       102|Aron|        1|       101|  2500|\n",
      "|       102|Aron|        2|       102|  1110|\n",
      "|       102|Aron|        3|       103|   500|\n",
      "|       102|Aron|        4|       104|   400|\n",
      "|       102|Aron|        5|       105|   150|\n",
      "|       102|Aron|        6|       106|   450|\n",
      "|       103| Sam|        1|       101|  2500|\n",
      "|       103| Sam|        2|       102|  1110|\n",
      "|       103| Sam|        3|       103|   500|\n",
      "|       103| Sam|        4|       104|   400|\n",
      "|       103| Sam|        5|       105|   150|\n",
      "|       103| Sam|        6|       106|   450|\n",
      "+----------+----+---------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customer.crossJoin(payment).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 52) Left-Semi-Join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This returns only the data from the left side that has a match on the right side based on the condition provided for the join statement. \n",
    "\n",
    "In contrast to Left join where all the rows from the Right side table are also present\n",
    "in the output, there is right side table data in the output. \n",
    "\n",
    "This can also be achieved in subquery kind of queries in conjunction with IN/EXISTS in SQL but using semi_join restricts the amount of data that is read from the right side table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+------+\n",
      "|paymentId|customerId|amount|\n",
      "+---------+----------+------+\n",
      "|        1|       101|  2500|\n",
      "|        2|       102|  1110|\n",
      "|        3|       103|   500|\n",
      "|        4|       104|   400|\n",
      "|        5|       105|   150|\n",
      "|        6|       106|   450|\n",
      "+---------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "payment.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+\n",
      "|customerId|name|\n",
      "+----------+----+\n",
      "|       101| Jon|\n",
      "|       102|Aron|\n",
      "|       103| Sam|\n",
      "+----------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customer.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+------+\n",
      "|paymentId|customerId|amount|\n",
      "+---------+----------+------+\n",
      "|        1|       101|  2500|\n",
      "|        3|       103|   500|\n",
      "|        2|       102|  1110|\n",
      "+---------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "payment.join(customer, payment(\"customerId\") === customer(\"customerId\"), \"leftsemi\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you look closely at the output, the joined output only consists of data from the Payment(Left) table which has a match for it in the Customer(Right) table. Rest of all the stuff is ignored. Also, note that the `name` column from the `Customer` table is not returned even for the matching `customerId`.This is really useful when you are trying to extract the only data in left that has a match on the right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Semi-Join can feel similar to Inner Join but the difference between them is that `Left Semi Join` only returns the records from the left-hand table, whereas the `Inner Join` returns the `columns` from both tables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 53) Left-anti-Join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the name suggests, it does exactly the opposite of Left semi-join. The output would just `return the data` that `doesn’t have a match` on the `right` side table. Only the columns on the left side table would be included in the result. Just the data filtered for the NOT IN condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+------+\n",
      "|paymentId|customerId|amount|\n",
      "+---------+----------+------+\n",
      "|        1|       101|  2500|\n",
      "|        2|       102|  1110|\n",
      "|        3|       103|   500|\n",
      "|        4|       104|   400|\n",
      "|        5|       105|   150|\n",
      "|        6|       106|   450|\n",
      "+---------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "payment.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+\n",
      "|customerId|name|\n",
      "+----------+----+\n",
      "|       101| Jon|\n",
      "|       102|Aron|\n",
      "|       103| Sam|\n",
      "+----------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customer.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+------+\n",
      "|paymentId|customerId|amount|\n",
      "+---------+----------+------+\n",
      "|        5|       105|   150|\n",
      "|        6|       106|   450|\n",
      "|        4|       104|   400|\n",
      "+---------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "payment.join(customer, payment(\"customerId\") === customer(\"customerId\"), \"leftanti\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 54) Self Join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In self join, we join the dataframe with itself. We have to make sure we are aliasing the dataframe so that we can access the individual columns without name collisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traditionally, self-joins are used to querying hierarchical data, comparing 2 attributes of the same table. In this example, we have the `employee` table which has data about the **employeeId** and his **manager**. We can use self join to get a view of data as if they are 2 different columns in the same table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "employee1: org.apache.spark.sql.DataFrame = [employeeId: int, employeeName: string ... 1 more field]\n"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val employee1 = spark.createDataFrame(Seq(\n",
    "  (1,\"ceo\",None),\n",
    "  (2,\"manager1\",Some(1)),\n",
    "  (3,\"manager2\",Some(1)),\n",
    "  (101,\"Amy\",Some(2)),\n",
    "  (102,\"Sam\",Some(2)),\n",
    "  (103,\"Aron\",Some(3)),\n",
    "  (104,\"Bobby\",Some(3)),\n",
    "  (105,\"Jon\", Some(3))\n",
    ")).toDF(\"employeeId\",\"employeeName\",\"managerId\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+---------+\n",
      "|employeeId|employeeName|managerId|\n",
      "+----------+------------+---------+\n",
      "|         1|         ceo|     null|\n",
      "|         2|    manager1|        1|\n",
      "|         3|    manager2|        1|\n",
      "|       101|         Amy|        2|\n",
      "|       102|         Sam|        2|\n",
      "|       103|        Aron|        3|\n",
      "|       104|       Bobby|        3|\n",
      "|       105|         Jon|        3|\n",
      "+----------+------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employee1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "selfJoinedEmp: org.apache.spark.sql.DataFrame = [employeeId: int, employeeName: string ... 4 more fields]\n"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val selfJoinedEmp = employee1.as(\"e\").join(employee1.as(\"m\"),$\"m.employeeId\" === $\"e.managerId\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+---------+----------+------------+---------+\n",
      "|employeeId|employeeName|managerId|employeeId|employeeName|managerId|\n",
      "+----------+------------+---------+----------+------------+---------+\n",
      "|         2|    manager1|        1|         1|         ceo|     null|\n",
      "|         3|    manager2|        1|         1|         ceo|     null|\n",
      "|       101|         Amy|        2|         2|    manager1|        1|\n",
      "|       102|         Sam|        2|         2|    manager1|        1|\n",
      "|       103|        Aron|        3|         3|    manager2|        1|\n",
      "|       104|       Bobby|        3|         3|    manager2|        1|\n",
      "|       105|         Jon|        3|         3|    manager2|        1|\n",
      "+----------+------------+---------+----------+------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "selfJoinedEmp.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This joined dataset has a lot of redundant data and doesn’t give us a clear picture of what our requirement is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+\n",
      "|employee|managerName|\n",
      "+--------+-----------+\n",
      "|manager1|        ceo|\n",
      "|manager2|        ceo|\n",
      "|     Amy|   manager1|\n",
      "|     Sam|   manager1|\n",
      "|    Aron|   manager2|\n",
      "|   Bobby|   manager2|\n",
      "|     Jon|   manager2|\n",
      "+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "selfJoinedEmp\n",
    ".select($\"e.employeeName\".as(\"employee\"),$\"m.employeeName\".as(\"managerName\"))\n",
    ".show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can select the required columns and alias them to make the output more understandable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 55) Joins on columns with nulls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s say we wanted to join on columns which have nulls in it. By default, Spark would skip these columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say I want to join, `df1` and `df2` on `id` column which has `nulls` in it. The result would not have the null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dftest1: org.apache.spark.sql.DataFrame = [id: int, name: string]\n"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dftest1 = Seq((Some(123),\"name1\"),(Some(456),\"name3\"),(None,\"name2\")).toDF(\"id\",\"name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dftest2: org.apache.spark.sql.DataFrame = [id: int, dept: string]\n"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dftest2 = Seq((None,\"sales\"),(Some(223),\"Legal\"),(Some(456),\"IT\")).toDF(\"id\",\"dept\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|  id| name|\n",
      "+----+-----+\n",
      "| 123|name1|\n",
      "| 456|name3|\n",
      "|null|name2|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dftest1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|  id| dept|\n",
      "+----+-----+\n",
      "|null|sales|\n",
      "| 223|Legal|\n",
      "| 456|   IT|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dftest2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----+\n",
      "| id| name|dept|\n",
      "+---+-----+----+\n",
      "|456|name3|  IT|\n",
      "+---+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dftest1.join(dftest2, Seq(\"id\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But let say we don’t want to lose that data,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+-----+\n",
      "| name|  id| dept|\n",
      "+-----+----+-----+\n",
      "|name3| 456|   IT|\n",
      "|name2|null|sales|\n",
      "+-----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dftest1.join(dftest2, dftest1(\"id\") <=> dftest2(\"id\")).drop(dftest1(\"id\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `<=>` operator is an Equality test operator that is safe to use when the columns have null values vs `===`. `<=>` returns the same result as the = operator for non-null operands, but returns true if both are null, false if one of them is null."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gotchas:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you join 2 columns, we generally ended having at-least 1 column duplicated if we join using the below signature. Here `id` is repeated twice. Either we have to drop that column or use another elegant way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+----+\n",
      "| id| name| id|dept|\n",
      "+---+-----+---+----+\n",
      "|456|name3|456|  IT|\n",
      "+---+-----+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dftest1.join(dftest2, dftest1.col(\"id\") === dftest2.col(\"id\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `join`S method in spark has a method that takes `usingColumns` as 1 of the parameter. When we use this method spark prevents duplicated columns when joining 2 dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----+\n",
      "| id| name|dept|\n",
      "+---+-----+----+\n",
      "|456|name3|  IT|\n",
      "+---+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dftest1.join(dftest2 , Seq(\"id\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only thing to take care is, this expects the name of the column supplied for the `usingColumns` should be present on both the sides, at-least the name of the column should be the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 56) Joins in Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the `joinWith` function to join 2 Datasets. This is similar to other join discussed above but the only difference is that `joinWith` preserves the type information of the resulting Dataset. It returns a `Dataset[(T, U)]` compared to `DataFrame` in all the above-mentioned joins which can be really precious if type-safety matters to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class Payments\n"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case class Payments(paymentId: Int, customerId: Int, amount:Int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class Customer\n"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case class Customer(customerId: Int, name: String)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "paymentDs: org.apache.spark.sql.Dataset[Payments] = [paymentId: int, customerId: int ... 1 more field]\n"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val paymentDs: Dataset[Payments] = payment.as[Payments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "customerDs: org.apache.spark.sql.Dataset[Customer] = [customerId: int, name: string]\n"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val customerDs: Dataset[Customer] = customer.as[Customer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+------+\n",
      "|paymentId|customerId|amount|\n",
      "+---------+----------+------+\n",
      "|        1|       101|  2500|\n",
      "|        2|       102|  1110|\n",
      "|        3|       103|   500|\n",
      "|        4|       104|   400|\n",
      "|        5|       105|   150|\n",
      "|        6|       106|   450|\n",
      "+---------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "paymentDs.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+\n",
      "|customerId|name|\n",
      "+----------+----+\n",
      "|       101| Jon|\n",
      "|       102|Aron|\n",
      "|       103| Sam|\n",
      "+----------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customerDs.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "x: org.apache.spark.sql.Dataset[(Customer, Payments)] = [_1: struct<customerId: int, name: string>, _2: struct<paymentId: int, customerId: int ... 1 more field>]\n"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val x: Dataset[(Customer, Payments)] \n",
    "        = customerDs.joinWith(paymentDs,paymentDs.col(\"customerId\") === customerDs.col(\"customerId\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+\n",
      "|         _1|            _2|\n",
      "+-----------+--------------+\n",
      "| [101, Jon]|[1, 101, 2500]|\n",
      "| [103, Sam]| [3, 103, 500]|\n",
      "|[102, Aron]|[2, 102, 1110]|\n",
      "+-----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you notice above, we get back a `Dataset[(Customer, Payments)]` when compared the join operation in the previous examples\n",
    "where we get a `Dataframe` back. This has all the types of join that are available for Dataframes as discussed above. 1 gotcha is that it is relatively tricky to use the returned object as it is a Dataset of Set of (Customer and Payment) object which can get a little tricky to work with after the join. Also, note that in case of missing records in Left or Right joins, the values are replaced by their respective null values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 57) Broadcast Hash Join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When 1 of the dataframe is small enough to fit in the memory, it is broadcasted over to all the executors where the larger dataset resides and a hash join is performed.\n",
    "\n",
    "This has 2 phase,\n",
    "broadcast-> the smaller dataset is broadcasted across the executors in the cluster where the larger table is located.\n",
    "hash join-> A standard hash join is performed on each executor.\n",
    "\n",
    "There is no shuffling involved in this and can be much quicker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>\n",
    "spark.sql.autoBroadcastJoinThreshold\n",
    "This can be configured to set the Maximum size in bytes for a dataframe to be broadcasted.\n",
    "-1 will disable broadcast join\n",
    "Default is 10485760 i.e., 10MB\n",
    "</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res112: String = 10485760\n"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get(\"spark.sql.autoBroadcastJoinThreshold\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `spark.broadcast.compress` can be used to configure whether to compress the data before sending it.\n",
    "- It uses the compression specified in the `spark.io.compression.codec` config and the default is lz4. We can use other compression codecs such as lz4,lzf, snappy, ZStandard.\n",
    "- `spark.broadcast.compress` is `true` by default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run explain to see if this type of join is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res113: org.apache.spark.sql.execution.SparkPlan =\r\n",
       "*(5) Project [customerId#378, name#379, paymentId#355, amount#357]\r\n",
       "+- *(5) SortMergeJoin [customerId#378], [customerId#356], Inner\r\n",
       "   :- *(2) Sort [customerId#378 ASC NULLS FIRST], false, 0\r\n",
       "   :  +- Exchange hashpartitioning(customerId#378, 200)\r\n",
       "   :     +- *(1) Project [_1#375 AS customerId#378, _2#376 AS name#379]\r\n",
       "   :        +- *(1) SerializeFromObject [assertnotnull(input[0, scala.Tuple2, true])._1 AS _1#375, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, assertnotnull(input[0, scala.Tuple2, true])._2, true, false) AS _2#376]\r\n",
       "   :           +- Scan ExternalRDDScan[obj#374]\r\n",
       "   +- *(4) Sort [customerId#356 ASC NULLS FIRST], false, 0\r\n",
       "      +- Exchange hashpartitioning(customerId#356, 200)\r",
       "..."
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customer.join(payment,Seq(\"customerId\")).queryExecution.executedPlan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can notice here that, even though my Dataframes are small in size sometimes spark doesn’t recognize that the size of the dataframe is < 10 MB. To enforce this we can use the `broadcast hint`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res114: org.apache.spark.sql.DataFrame = [customerId: int, name: string ... 2 more fields]\n"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customer.join(broadcast(payment),Seq(\"customerId\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res115: org.apache.spark.sql.execution.SparkPlan =\r\n",
       "*(2) Project [customerId#378, name#379, paymentId#355, amount#357]\r\n",
       "+- *(2) BroadcastHashJoin [customerId#378], [customerId#356], Inner, BuildRight\r\n",
       "   :- *(2) Project [_1#375 AS customerId#378, _2#376 AS name#379]\r\n",
       "   :  +- *(2) SerializeFromObject [assertnotnull(input[0, scala.Tuple2, true])._1 AS _1#375, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, assertnotnull(input[0, scala.Tuple2, true])._2, true, false) AS _2#376]\r\n",
       "   :     +- Scan ExternalRDDScan[obj#374]\r\n",
       "   +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[1, int, false] as bigint)))\r\n",
       "      +- *(1) Project [_1#351 AS paymentId#355, _2#352 AS customerId#356, _3#353 AS amount#357]\r\n",
       "         +- *(1) SerializeFromObject [ass..."
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customer.join(broadcast(payment),Seq(\"customerId\")).queryExecution.executedPlan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a broadcast hint is specified, the join side with the hint will be broadcasted irrespective of autoBroadcastJoinThreshold. If both sides of the join have the broadcast hints, the side with a smaller `estimated physical size` will be broadcasted. If there is no hint and if the estimated physical size of the table < autoBroadcastJoinThreshold then that table is broadcasted to all the executor nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark has a BitTorrent-like implementation to perform broadcast. This is to avoid the driver being the bottleneck when sending data to multiple executors. In this, the broadcasting table is serialized and divided into small chunks by the driver and is stored in the BlockManager of the driver. In each executor, the executor first tries to locate the object from its BlockManager, remote fetch smaller chunks of the object from the driver and/or others executors if they are available in the other executors’ BlockManager."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once an executor has the chunk, it puts the chunks in its BlockManager for other executors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normally, `BHJ can perform faster` than other join algorithms when the broadcast side is small enough. However, broadcasting tables is a network-intensive operation and it can cause OOM sometimes or even perform worse than other algorithms when the table broadcasted is big enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BHJ is not supported for a full outer join. For right outer join, the only left side table can be broadcasted and for other left joins only right table can be broadcasted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 58) Shuffle Hash Join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shuffle hash join has 2 phases:\n",
    "\n",
    " 1. `A shuffle phase`, where the data from the join tables are partitioned based on the join key. What this phase does shuffles data across the partitions. The idea here is that if 2 tables have the same keys, they end up in the same partition so that the data required for joins is available in the same partition.\n",
    "\n",
    "2. `Hash join phase`: The data on each partition performs a classic single node hash join algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What Shuffle hash join does is that breaks apart 1 big join of 2 tables into smaller chunks of the localized relatively smaller branch. Shuffle is a very expensive operation as it involves a lot of network-intensive movement of the data across the nodes in the cluster. Also, note that the creation of Hash tables is also an expensive operation and is memory bound. This is not well suited for joins that wouldn’t fit in memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, note that the performance of this is based on the `distribution of keys` in the dataset. The greater number of unique join keys the better data distribution we get. The maximum amount of parallelism that we can achieve is proportional to the `number of unique keys`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say we are joining 2 datasets based on something which would be unique like `empId` would be a good candidate over something like `DepartmentName` which wouldn’t have a lot of unique keys and would limit the maximum parallelism that we could achieve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gotchas:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to be cognizant about the situation in which 1 single partition(a small subset of all the partitions) getting way too much of data over the other partition. This can happen when there is a skew in our dataset. The distribution of the data should be uniform across the cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say we are joining Stock data on companies stock ticker and each partition gets data based on the company’s name. But there might not be a whole lot of data in the partition that house stocks of Z but there might be way too much of data on the partition that would house A such as Apple and Amazon. We would have to do something called as `salting` in these kinds of situations. Salting is a technique of adding some randomness in non-unique keys. Say we have 1 Million Amazon stocks in our dataset and Zillow stocks are 100. We selectively add a random integer to the Amazon stock so that the hash of the Amazon data is further uniformly distributed among different partitions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The main thing to note here is that Shuffle Hash join will be used as the join strategy only when **`spark.sql.join.preferSortMergeJoin`** is set to `false`\n",
    "and the cost to build a hash map is less than sorting the data. By default, sort merge join is preffered over Shuffle Hash Join.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ShuffledHashJoin is still useful when:\n",
    "\n",
    "* any partition of the build side could `fit in memory`\n",
    "* one table is much smaller than the other one, then `cost to build a hash table` on a smaller table is smaller than sorting the larger table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 59) Sort merge join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sort merge join is the default join strategy** if the matching join keys are sortable and not eligible for broadcast join or shuffle hash join. It is a very scalable approach and performs better than other joins most of the times. It has its traits from the legendary map-reduce programs. What makes it scalable is that it can spill the data to the disk and doesn’t require the entire data to fit inside the memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It has 3 phases:\n",
    "\n",
    "* `Shuffle Phase`: The 2 large tables are repartitioned as per the join keys across the partitions in the cluster.\n",
    "\n",
    "* `Sort Phase`: Sort the data within each partition parallelly.\n",
    "\n",
    "* `Merge Phase`: Join the 2 sorted + partitioned data. This is basically merging of the dataset by iterating over the elements and joining the rows having the same value for the join key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, note that sometimes spark by default chooses Merge Sort Join which might not be always ideal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we have a customer Dataframe and a payment dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.util.SizeEstimator\n"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.util.SizeEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47.767576\n"
     ]
    }
   ],
   "source": [
    "println(SizeEstimator.estimate(payment)/1000000.00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47.770096\n"
     ]
    }
   ],
   "source": [
    "println(SizeEstimator.estimate(customer)/1000000.00)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both of the data frames are almost 47.5MB. By default, spark chooses Sort-Merge Join but sometimes a Broadcast join can do better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res118: org.apache.spark.sql.execution.SparkPlan =\r\n",
       "*(5) Project [customerId#378, name#379, paymentId#355, amount#357]\r\n",
       "+- *(5) SortMergeJoin [customerId#378], [customerId#356], Inner\r\n",
       "   :- *(2) Sort [customerId#378 ASC NULLS FIRST], false, 0\r\n",
       "   :  +- Exchange hashpartitioning(customerId#378, 200)\r\n",
       "   :     +- *(1) Project [_1#375 AS customerId#378, _2#376 AS name#379]\r\n",
       "   :        +- *(1) SerializeFromObject [assertnotnull(input[0, scala.Tuple2, true])._1 AS _1#375, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, assertnotnull(input[0, scala.Tuple2, true])._2, true, false) AS _2#376]\r\n",
       "   :           +- Scan ExternalRDDScan[obj#374]\r\n",
       "   +- *(4) Sort [customerId#356 ASC NULLS FIRST], false, 0\r\n",
       "      +- Exchange hashpartitioning(customerId#356, 200)\r",
       "..."
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customer.join(payment,\"customerId\").queryExecution.executedPlan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+---------+------+\n",
      "|customerId|name|paymentId|amount|\n",
      "+----------+----+---------+------+\n",
      "|       101| Jon|        1|  2500|\n",
      "|       103| Sam|        3|   500|\n",
      "|       102|Aron|        2|  1110|\n",
      "+----------+----+---------+------+\n",
      "\n",
      "Time taken: 637 ms\n"
     ]
    }
   ],
   "source": [
    "spark.time(customer.join(payment,\"customerId\").show)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we provide the broadcast hint,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res120: org.apache.spark.sql.execution.SparkPlan =\r\n",
       "*(2) Project [customerId#378, name#379, paymentId#355, amount#357]\r\n",
       "+- *(2) BroadcastHashJoin [customerId#378], [customerId#356], Inner, BuildRight\r\n",
       "   :- *(2) Project [_1#375 AS customerId#378, _2#376 AS name#379]\r\n",
       "   :  +- *(2) SerializeFromObject [assertnotnull(input[0, scala.Tuple2, true])._1 AS _1#375, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, assertnotnull(input[0, scala.Tuple2, true])._2, true, false) AS _2#376]\r\n",
       "   :     +- Scan ExternalRDDScan[obj#374]\r\n",
       "   +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[1, int, false] as bigint)))\r\n",
       "      +- *(1) Project [_1#351 AS paymentId#355, _2#352 AS customerId#356, _3#353 AS amount#357]\r\n",
       "         +- *(1) SerializeFromObject [ass..."
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customer.join(broadcast(payment),\"customerId\").queryExecution.executedPlan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+---------+------+\n",
      "|customerId|name|paymentId|amount|\n",
      "+----------+----+---------+------+\n",
      "|       101| Jon|        1|  2500|\n",
      "|       102|Aron|        2|  1110|\n",
      "|       103| Sam|        3|   500|\n",
      "+----------+----+---------+------+\n",
      "\n",
      "Time taken: 129 ms\n"
     ]
    }
   ],
   "source": [
    "spark.time(customer.join(broadcast(payment),\"customerId\").show)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to remember that there is no single right answer to a problem. The right answer is always `it depends`. We have to utilize spark’s features generously as per our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: `SizeEstimator.estimate` gives an estimate of how much space the object takes up on the JVM heap. This will often be higher than the typical serialized size of the object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gotchas:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Ideal performance of Sort-Merge join, it is important that all rows having the same value for the join key are available in the same partition. This warrants for the infamous partition exchange(shuffle) between executors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collocated partitions can avoid unnecessary data shuffle. Data needs to be evenly distributed n the join keys. The number of join keys is unique enough so that they can be equally distributed across the cluster to achieve the max parallelism from the available partitions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cartesian Join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When no join keys are specified and the join type is an inner join, CartesianProduct is chosen. This is an inherently very expensive operation and should not be chosen unless necessary. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 60) BroadcastNestedLoopJoin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This type of join is chosen when no joining keys are specified and either has a broadcast hint or if 1 side of the join could be broadcasted and is less than **spark.sql.autoBroadcastJoinThreshold**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This type of join is also the final fall back option if no join keys are specified, and is not an inner join. But this could be very slow and can cause **OutOfMemoryExceptions!** if the broadcasted dataset is large enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Takeaways:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sort-Merge join is the default join and performs well in most of the scenarios. And for cases, if you are confident enough that Shuffle Hash join is better than Sort-Merge join, disable Sort-Merge join for those scenarios. However, when the build size is smaller than the stream size, Shuffle Hash join will do better than Sort-Merge join.\n",
    "\n",
    "\n",
    "- Tune the **spark.sql.autoBroadcastJoinThreshold** accordingly if deemed necessary. Try to use Broadcast joins wherever possible and filter out the irrelevant rows to the join key before the join to avoid unnecessary data shuffling.\n",
    "\n",
    "\n",
    "- Joins without unique join keys or **no join keys** can often be **very expensive** and should be avoided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joins in RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "myrdd1: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[412] at parallelize at <console>:31\n"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val myrdd1 = sc.parallelize(Seq((\"math\",55),(\"math\",56),(\"english\",57),(\"english\",58),(\"science\",59),(\"science\",54)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "myrdd2: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[413] at parallelize at <console>:31\n"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val myrdd2 = sc.parallelize(Seq((\"math\",60),(\"math\",65),(\"science\",61),(\"science\",62),(\"history\",63),(\"history\",64)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res122: Array[(String, Int)] = Array((math,55), (math,56), (english,57), (english,58), (science,59), (science,54))\n"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myrdd1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res123: Array[(String, Int)] = Array((math,60), (math,65), (science,61), (science,62), (history,63), (history,64))\n"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myrdd2.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 61) Join (RDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `join()` is transformation.\n",
    "- It’s in package `org.apache.spark.rdd.pairRDDFunction`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returns an RDD containing all pairs of elements with matching keys in this and other.\n",
    "\n",
    "Each pair of elements will be returned as a `(k, (v1, v2))` tuple, where `(k, v1)` is in this and `(k, v2)` is in other. \n",
    "\n",
    "Performs a `hash join` across the cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "joined: org.apache.spark.rdd.RDD[(String, (Int, Int))] = MapPartitionsRDD[416] at join at <console>:35\n"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val joined = myrdd1.join(myrdd2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res124: Array[(String, (Int, Int))] = Array((math,(55,60)), (math,(55,65)), (math,(56,60)), (math,(56,65)), (science,(59,61)), (science,(59,62)), (science,(54,61)), (science,(54,62)))\n"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 62) leftOuterJoin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `leftOuterJoin()` is transformation.\n",
    "- It’s in package `org.apache.spark.rdd.pairRDDFunction`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performs a left outer join of this and other.\n",
    "\n",
    "For each element `(k, v)` in this, the resulting RDD will either contain all pairs `(k, (v, Some(w)))` for `w` in other, or the pair `(k, (v, None))` if no elements in other have key `k`.\n",
    "\n",
    "`Hash-partitions` the output using the existing partitioner/parallelism level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`leftOuterJoin()` performs a join between two RDDs where the keys must be present in first RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res125: Array[(String, Int)] = Array((math,55), (math,56), (english,57), (english,58), (science,59), (science,54))\n"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myrdd1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res126: Array[(String, Int)] = Array((math,60), (math,65), (science,61), (science,62), (history,63), (history,64))\n"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myrdd2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "leftJoined: org.apache.spark.rdd.RDD[(String, (Int, Option[Int]))] = MapPartitionsRDD[419] at leftOuterJoin at <console>:35\n"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val leftJoined = myrdd1.leftOuterJoin(myrdd2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res127: Array[(String, (Int, Option[Int]))] = Array((math,(55,Some(60))), (math,(55,Some(65))), (math,(56,Some(60))), (math,(56,Some(65))), (science,(59,Some(61))), (science,(59,Some(62))), (science,(54,Some(61))), (science,(54,Some(62))), (english,(57,None)), (english,(58,None)))\n"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leftJoined.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 63) rightOuterJoin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `rightOuterJoin()` is transformation.\n",
    "- It’s in package `org.apache.spark.rdd.pairRDDFunction`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performs a right outer join of this and other. \n",
    "\n",
    "For each element `(k, w)` in other, the resulting RDD will either contain all pairs `(k, (Some(v), w))` for `v` in this, or the pair `(k, (None, w))` if no elements in this have key `k`. \n",
    "\n",
    "`Hash-partitions` the resulting RDD using the existing partitioner/parallelism level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It performs the join between two RDDs where the key must be present in other RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res128: Array[(String, Int)] = Array((math,55), (math,56), (english,57), (english,58), (science,59), (science,54))\n"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myrdd1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res129: Array[(String, Int)] = Array((math,60), (math,65), (science,61), (science,62), (history,63), (history,64))\n"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myrdd2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rightJoined: org.apache.spark.rdd.RDD[(String, (Option[Int], Int))] = MapPartitionsRDD[422] at rightOuterJoin at <console>:35\n"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rightJoined = myrdd1.rightOuterJoin(myrdd2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res130: Array[(String, (Option[Int], Int))] = Array((math,(Some(55),60)), (math,(Some(55),65)), (math,(Some(56),60)), (math,(Some(56),65)), (history,(None,63)), (history,(None,64)), (science,(Some(59),61)), (science,(Some(59),62)), (science,(Some(54),61)), (science,(Some(54),62)))\n"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rightJoined.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 64) Cartesian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provides cartesian product b/w 2 RDDs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res131: Array[(String, Int)] = Array((math,55), (math,56), (english,57), (english,58), (science,59), (science,54))\n"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myrdd1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res132: Array[(String, Int)] = Array((math,60), (math,65), (science,61), (science,62), (history,63), (history,64))\n"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myrdd2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cartesianJoined: org.apache.spark.rdd.RDD[((String, Int), (String, Int))] = CartesianRDD[423] at cartesian at <console>:35\n"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val cartesianJoined = myrdd1.cartesian(myrdd2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res133: Array[((String, Int), (String, Int))] = Array(((math,55),(math,60)), ((math,55),(math,65)), ((math,55),(science,61)), ((math,55),(science,62)), ((math,55),(history,63)), ((math,55),(history,64)), ((math,56),(math,60)), ((english,57),(math,60)), ((math,56),(math,65)), ((math,56),(science,61)), ((english,57),(math,65)), ((english,57),(science,61)), ((math,56),(science,62)), ((english,57),(science,62)), ((math,56),(history,63)), ((math,56),(history,64)), ((english,57),(history,63)), ((english,57),(history,64)), ((english,58),(math,60)), ((english,58),(math,65)), ((english,58),(science,61)), ((english,58),(science,62)), ((english,58),(history,63)), ((english,58),(history,64)), ((science,59),(math,60)), ((science,54),(math,60)), ((science,59),(math,65)), ((science,59),(science,61)), ..."
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cartesianJoined.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 65) Tungsten Framework purpose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Tungsten`** is the codename for the umbrella project to make changes to Apache Spark’s execution engine that focuses on substantially improving the efficiency of memory and CPU for Spark applications, to push performance closer to the limits of modern hardware. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tungsten focuses on the hardware architecture of the platform Spark runs on, including but not limited to JVM, LLVM, GPU, NVRAM, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../Resources/Tungsten.png\" style=\"height:230px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This effort includes the following initiatives:\n",
    "\n",
    " - **Memory Management and Binary Processing:** leveraging application semantics to manage memory explicitly and eliminate the overhead of JVM object model and garbage collection\n",
    " \n",
    " \n",
    " - **Cache-aware computation:** algorithms and data structures to exploit memory hierarchy\n",
    " \n",
    " \n",
    " - **Code generation:** using code generation to exploit modern compilers and CPUs\n",
    " \n",
    " \n",
    " - **No virtual function dispatches:** this reduces multiple CPU calls which can have a profound impact on performance when dispatching billions of times.\n",
    " \n",
    " \n",
    " - **Intermediate data in memory vs CPU registers:** Tungsten Phase 2 places intermediate data into CPU registers. This is an order of magnitudes reduction in the number of cycles to obtain data from the CPU registers instead of from memory.\n",
    " \n",
    " \n",
    " - **Loop unrolling and SIMD:** Optimize Apache Spark’s execution engine to take advantage of modern compilers and CPUs’ ability to efficiently compile and execute simple for loops (as opposed to complex function call graphs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The focus on CPU efficiency is motivated by the fact that Spark workloads are increasingly bottlenecked by CPU and memory use rather than IO and network communication. The trend is shown by recent research on the performance of big data workloads."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**References:**\n",
    "\n",
    "[Reference-1](https://community.cloudera.com/t5/Community-Articles/What-is-Tungsten-for-Apache-Spark/ta-p/248445)\n",
    "\n",
    "[Reference-2](https://databricks.com/glossary/tungsten)\n",
    "\n",
    "[Reference-3](https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 66) Catalyst Framework purpose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the core of Spark SQL is the **`Catalyst optimizer`**, which leverages advanced programming language features (e.g. Scala’s pattern matching and quasi quotes) in a novel way to build an extensible query optimizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Catalyst is based on functional programming constructs in Scala and designed with these key two purposes:\n",
    "\n",
    "* Easily add new optimization techniques and features to Spark SQL\n",
    "\n",
    "* Enable external developers to extend the optimizer (e.g. adding data source specific rules, support for new data types, etc.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../Resources/Catalyst-Optimizer-diagram.png\" style=\"height:150px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Catalyst contains a general library for representing trees and applying rules to manipulate them. On top of this framework, it has libraries specific to relational query processing (e.g., expressions, logical query plans), and several sets of rules that handle different phases of query execution: analysis, logical optimization, physical planning, and code generation to compile parts of queries to Java bytecode. For the latter, it uses another Scala feature, quasiquotes, that makes it easy to generate code at runtime from composable expressions. Catalyst also offers several public extension points, including external data sources and user-defined types. As well, Catalyst supports both rule-based and cost-based optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**References:**\n",
    "\n",
    "[Reference-1](https://data-flair.training/blogs/spark-sql-optimization/)\n",
    "\n",
    "[Reference-2](https://databricks.com/glossary/catalyst-optimizer)\n",
    "\n",
    "[Reference-3](https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 67) Akka Framework purpose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Akka is a general purpose framework to create reactive, distributed, parallel and resilient concurrent applications in Scala or Java. Akka uses the Actor model to hide all the thread-related code and gives you really simple and helpful interfaces to implement a scalable and fault-tolerant system easily. A good example for Akka is a real-time application that consumes and process data coming from mobile phones and sends them to some kind of storage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Earlier Spark used to depend on Akka as it used to rely on Akka toolkit to communicate between nodes. As of Spark 1.6 , Spark no longer depends on Akka."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Akka is an actor framework for the jvm. It's is based on erlang and supports actor based distributed concurrency. The Actor Model provides a higher level of abstraction for writing concurrent and distributed applications. It helps to developer to deals with explicit locking and thread management. Akka makes it easier to write correct concurrent and parallel application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Major use cases :\n",
    "\n",
    "* Transaction processing\n",
    "* Concurrency/parallelism\n",
    "* Simulation\n",
    "* Batch processing\n",
    "* Gaming and Betting\n",
    "* Complex Event Stream Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Reference](https://intellipaat.com/community/7403/apache-spark-vs-akka)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 68) Difference between RDD and Dataframe ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RDD:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be termed as building block of spark. Internal final computation is always done on RDDs no matter which of the abstraction DataFrame or Dataset is used, it is the vital part. One of the most advantageous things about RDD is its simplicity, it provides us with familiar OOP style APIs. RDD can also be easily cached if some data is to be reevaluated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataFrame:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrame can simply be defined as an abstraction which gives a schema view of data. We can think of the data in DataFrame like a table in database. But It works only on structured and semi-structured data, it offers huge performance improvement over RDDs because of features like Custom Memory management and Optimized Execution Plans."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Differences:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*  RDD provides a more familiar OOP type programming style with compile    time safety, while DataFrame detects attribute error only at runtime.\n",
    "\n",
    "\n",
    "* No inbuilt optimization engine is available in case of RDD while the DataFrame optimization takes place using Catalyst optimizer.\n",
    "\n",
    "\n",
    "* Incase of RDD whenever the data needs to be distributed within the cluster or written to the disk, it is done using Java serialization. There is no need to use java serialization to encode the data in case of DataFrame.\n",
    "\n",
    "\n",
    "* Efficiency in case of RDD is less than DataFrame because serialization needs to be performed individually on the objects which takes more time.\n",
    "\n",
    "\n",
    "* RDD is slower in performing simple grouping and aggregation operations as compared to DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Reference](https://intellipaat.com/community/141/what-is-the-difference-between-rdd-and-dataframes-in-apache-spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 69) Difference between Parquet,  AVRO and ORC file ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In simplest word, these all are file formats.\n",
    "\n",
    "Hadoop like big storage and data processing ecosystem need optimized read and write performance oriented data formats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../Resources/data-formats-300x145.png\" style=\"height:150px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AVRO:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* It is row major format.\n",
    "* Its primary design goal was schema evolution.\n",
    "* In the avro format, we store schema separately from data. Generally avro schema file `(.avsc)` is maintained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ORC:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Column oriented storage format.\n",
    "* Originally it is `Hive's Row Columnar` file. Now improved as `Optimized RC (ORC)`.\n",
    "* Schema is with the data, but as a part of footer.\n",
    "* Data is stored as row groups and stripes.\n",
    "* Each stripe maintains indexes and stats about data it stores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../Resources/orc-file-structure.png\" style=\"width:450px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parquet:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Similar to ORC. Based on google dremel\n",
    "* Schema stored in footer\n",
    "* Column oriented storage format\n",
    "* Has integrated compression and indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the basic difference between and Parquet and ORC is that ORC use snappy for data compression so the data is more compressed in ORC compared to Avro.\n",
    "\n",
    "Basically ORC is best for retrieving data and compressing data as compare to Parquet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../Resources/Nexla-File-Format.png\" style=\"width:450px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**References:**\n",
    "\n",
    "[Reference-1](https://www.datanami.com/2018/05/16/big-data-file-formats-demystified/)\n",
    "\n",
    "[Reference-2](https://www.quora.com/What-are-the-differences-between-ORC-Avro-and-Parquet-File-Formats-in-Hadoop-in-terms-of-compression-and-speed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 70) Same operation can be done through both RDD and Dataframe. Which one you will prefer and why ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DataFrame**\n",
    "\n",
    "Reasons:\n",
    "\n",
    "* Incase of RDD whenever the data needs to be distributed within the cluster or written to the disk, it is done using Java serialization. There is no need to use java serialization to encode the data in case of DataFrame.\n",
    "\n",
    "\n",
    "* Efficiency in case of RDD is less than DataFrame because serialization needs to be performed individually on the objects which takes more time.\n",
    "\n",
    "\n",
    "* RDD is slower in performing simple grouping and aggregation operations as compared to DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 71) How Spark provides high availability?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can say any system highly available if its downtime is tolerable. This time depends on how critical the application is. Zero down time is an imaginary term for any system. Consider any machine has an uptime of 97.7%, so its probability to go down will be 0.023. If we have similar two machines, then the probability of both of them going down will be (0.023*0.023). in most high availability environment we have three machines in use, in that case, the probability of going down is (0.023*0.023*0.023) i.e. 0.000012167, which guarantees an uptime of system to be  99.9987833%  which is highly acceptable uptime guarantee. 6. Apache Spark High Availability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note:\n",
    "\n",
    "To setup high availability in Spark:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Components in play"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a reminder, here are the components in play to run an application:\n",
    "\n",
    "The cluster:\n",
    "\n",
    "* **Spark Master:** coordinates the resources\n",
    "* **Spark Workers:** offer resources to run the applications\n",
    "\n",
    "The application:\n",
    "\n",
    "* **Driver:** the part of the application that coordinates the processing\n",
    "* **Executors:** the distributed part of the application that process the data\n",
    "\n",
    "When the *driver* is run in *cluster mode*, it runs on a *worker*.\n",
    "\n",
    "Notice that each component run its own JVM: the *workers* spawn separate JVMs to run the *driver* and the *executors*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fault tolerance:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a Spark *standalone* cluster, here is what happens if the JVM running a component dies:\n",
    "\n",
    "* Master: ❌ can no longer run new applications and the UI becomes unavailable.\n",
    "* Worker: ✅ not a problem, the cluster simply has less resources.\n",
    "* Driver: ❌ the application dies.\n",
    "* Executor: ✅ not a problem, the partitions being processed are sent to another executor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that losing a JVM or losing the whole EC2 instance has the same effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how to deal with these problems:\n",
    "\n",
    "*Master* -> setup a *standby master*.\n",
    "\n",
    "*Driver* -> run the application in *supervised mode*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up a standby master"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the Master is a single point of failure, Spark offers the ability to start another instance of a master which will be in standby until the active master disappears. When the standby master becomes the active master, the workers will reconnect to this master and existing applicatione will continue running without problem.\n",
    "\n",
    "This functionality relies on ZooKeeper to perform master election.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running the application in supervised mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The supervised mode allows the driver to be restarted on a different node if it dies. Enabling this functionality simply requires adding the `--supervise` flag when running `spark-submit`:\n",
    "\n",
    "`spark-submit --supervise ...`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Reference - 1](https://gist.github.com/aseigneurin/3af6b228490a8deab519c6aea2c209bc)\n",
    "\n",
    "[Reference - 2](https://data-flair.training/blogs/fault-tolerance-in-apache-spark/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 72) schemaRDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **`SchemaRDDs`** are composed Row objects along with a schema that describes the data types of each column in the row.\n",
    "\n",
    "- A SchemaRDD is similar to a table in a traditional relational database.\n",
    "\n",
    "- A SchemaRDD can be created from an existing RDD, Parquet file, a JSON dataset, or by running HiveQL against data stored in Apache Hive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>\n",
    "// One method for defining the schema of an RDD is to make a case class with the desired column\n",
    "// names and types.\n",
    "case class Record(key: Int, value: String)\n",
    "\n",
    "val sc: SparkContext // An existing spark context.\n",
    "val sqlContext = new SQLContext(sc)\n",
    "\n",
    "// Importing the SQL context gives access to all the SQL functions and implicit conversions.\n",
    "import sqlContext._\n",
    "\n",
    "val rdd = sc.parallelize((1 to 100).map(i => Record(i, s\"val_$i\")))\n",
    "// Any RDD containing case classes can be registered as a table.  The schema of the table is\n",
    "// automatically inferred using scala reflection.\n",
    "rdd.registerTempTable(\"records\")\n",
    "\n",
    "val results: SchemaRDD = sql(\"SELECT * FROM records\")\n",
    "</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 73) What is the difference between schemaRDD and Dataframe ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`SchemaRDD` in `Spark 1.2` has been `replaced by DataFrames` in `Spark 1.3`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 74) Lazy evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the name itself indicates its definition, **`lazy evaluation`** in Spark means that the execution will not start until an action is triggered. \n",
    "\n",
    "In Spark, the picture of lazy evaluation comes when Spark `transformations` occur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../Resources/lazy-evaluation-in-apache-spark-768x402-1.jpg\" style=\"height:300px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Transformations** are lazy in nature meaning when we call some operation in RDD, it does not execute immediately.\n",
    "\n",
    "Spark maintains the record of which operation is being called(Through DAG). We can think Spark RDD as the data, that we built up through transformation.\n",
    "\n",
    "Since transformations are lazy in nature, so we can execute operation any time by calling an action on data. Hence, in lazy evaluation data is not loaded until it is necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../Resources/apache-spark-lazy-evaluation.gif\" style=\"height:300px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 75) Advantages of Lazy Evaluation in Spark Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some benefits of Lazy evaluation in Apache Spark-\n",
    "\n",
    "- **Increases Manageability:**\n",
    "By lazy evaluation, users can organize their Apache Spark program into smaller operations. It reduces the number of passes on data by grouping operations.\n",
    "\n",
    "\n",
    "- **Saves Computation and increases Speed**\n",
    "Spark Lazy Evaluation plays a key role in saving calculation overhead. Since only necessary values get compute. It saves the trip between driver and cluster, thus speeds up the process.\n",
    "\n",
    "\n",
    "- **Reduces Complexities**\n",
    "The two main complexities of any operation are time and space complexity. Using Apache Spark lazy evaluation we can overcome both. Since we do not execute every operation, Hence, the time gets saved. It let us work with an infinite data structure. The action is triggered only when the data is required, it reduces overhead.\n",
    "\n",
    "\n",
    "- **Optimization**\n",
    "It provides optimization by reducing the number of queries. Learn more about Apache Spark Optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, Lazy evaluation enhances the power of Apache Spark by reducing the execution time of the RDD operations. It maintains the lineage graph to remember the operations on RDD. As a result, it Optimizes the performance and achieves fault tolerance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 76) Fault tolerance in Apache Spark – Reliable Spark Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will see fault-tolerant stream processing with Spark Streaming and Spark RDD fault tolerance. We will also learn what is Spark Streaming write ahead log, Spark streaming driver failure, Spark streaming worker failure to understand how to achieve fault tolerance in Apache Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../Resources/Apache-Spark-Fault-Tolerance-01.jpg\" style=\"height:250px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fault:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fault refers to failure, thus fault tolerance in Apache Spark is the capability to operate and to recover loss after a failure occurs. \n",
    "\n",
    "If we want our system to be fault tolerant, it should be redundant because we require a redundant component to obtain the lost data. The faulty data recovers by redundant data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spark RDD Fault Tolerance:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark operates on data in fault-tolerant file systems like HDFS or S3. So all the RDDs generated from fault tolerant data is fault tolerant. \n",
    "\n",
    "But this does not set true for streaming/live data (data over the network). So the key need of fault tolerance in Spark is for this kind of data. \n",
    "\n",
    "The basic fault-tolerant semantic of Spark are:\n",
    "\n",
    "* Since Apache Spark RDD is an immutable dataset, each Spark RDD remembers the lineage of the deterministic operation that was used on fault-tolerant input dataset to create it.\n",
    "\n",
    "\n",
    "* If due to a worker node failure any partition of an RDD is lost, then that partition can be re-computed from the original fault-tolerant dataset using the lineage of operations.\n",
    "\n",
    "\n",
    "* Assuming that all of the RDD transformations are deterministic, the data in the final transformed RDD will always be the same irrespective of failures in the Spark cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To achieve fault tolerance for all the generated RDDs, the achieved data replicates among multiple Spark executors in worker nodes in the cluster. \n",
    "\n",
    "This results in two types of data that needs to recover in the event of failure:\n",
    "\n",
    "* **Data received and replicated** – In this, the data gets replicated on one of the other nodes thus the data can be retrieved when a failure.\n",
    "\n",
    "\n",
    "* **Data received but buffered for replication** – The data is not replicated thus the only way to recover fault is by retrieving it again from the source."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../Resources/Fault-Tolerance-in-Apache-Spark-min-1.jpg\" style=\"height:300px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Failure also occurs in worker as well as driver nodes:\n",
    "\n",
    "* **Failure of worker node** – The node which runs the application code on the Spark cluster is Spark worker node. These are the slave nodes. Any of the worker nodes running executor can fail, thus resulting in loss of in-memory If any receivers were running on failed nodes, then their buffer data will be lost.\n",
    "\n",
    "\n",
    "* **Failure of driver node** – If there is a failure of the driver node that is running the Spark Streaming application, then SparkContent losses and all executors lose their in-memory data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Apache Mesos` helps in making the Spark master fault tolerant by maintaining the backup masters.\n",
    "\n",
    "It is open source software residing between the application layer and the operating system.\n",
    "\n",
    "It makes easier to deploy and manage applications in large-scale clustered environment.\n",
    "\n",
    "Executors are relaunched if they fail.\n",
    "\n",
    "Post failure, executors are relaunched automatically and spark streaming does parallel recovery by recomputing Spark RDD’s on input data.\n",
    "\n",
    "Receivers are restarted by the workers when they fail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fault Tolerance with Receiver-based sources:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For input sources based on receivers, the fault tolerance depends on both- the **failure scenario** and the **type of receiver**.\n",
    "\n",
    "There are two types of receiver:\n",
    "\n",
    "* **Reliable receiver** – Once we ensure that the received data replicates, the reliable sources are acknowledged. If the receiver fails, the source will not receive acknowledgment for the buffered data. So, the next time restarts the receiver, the source will resend the data. Hence, no data will lose due to failure.\n",
    "\n",
    "\n",
    "* **Unreliable Receiver** – Due to the worker or driver failure, the data can loss sincethe receiver does not send an acknowledgment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If the worker node fails**, and the receiver is reliable there will be no data loss. But in the case of unreliable receiver data loss will occur. With the unreliable receiver, data received but not replicated can be lost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spark Streaming write ahead logs:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If the driver node fails**, all the data that was received and replicated in memory will be lost. This will affect the result of the stateful transformation. To avoid the loss of data, Spark 1.2 introduced **write ahead logs**, which save received data to fault-tolerant storage. All the data received is written to write ahead logs before it can be processed to Spark Streaming.\n",
    "Write ahead logs are used in database and file system. It ensure the durability of any data operations. It works in the way that first the intention of the operation is written down in the durable log. After this, the operation is applied to the data. This is done because if the system fails in the middle of applying the operation, the lost data can be recovered. It is done by reading the log and reapplying the data it has intended to do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:70%\">\n",
    "<thead>\n",
    "<tr><th>Deployment Scenario\t</th><th>Worker Failure\t</th><th>Driver Failure</th></tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr><td>Spark 1.1 or earlier</td><td>Buffered data lost with unreliable receivers</td><td>Buffered data lost with the unreliable receiver.</td></tr>\n",
    "<tr><td>Spark 1.2 or later without write ahead logs</td><td>Zero data loss with reliable receivers, At-least-once semantics</td><td>Past data lost with all receivers, Undefined semantics</td></tr>\n",
    "<tr><td>Spark 1.2 or later with write ahead logs</td><td>Zero data loss with reliable receivers, At-least-once semantics</td><td>Zero data loss with reliable receivers and files, At-least-once semantics</td></tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### High Availability:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can say any system highly available if its downtime is tolerable. This time depends on how critical the application is. Zero down time is an imaginary term for any system. Consider any machine has an uptime of 97.7%, so its probability to go down will be 0.023. If we have similar two machines, then the probability of both of them going down will be (0.023*0.023). in most high availability environment we have three machines in use, in that case, the probability of going down is (0.023*0.023*0.023) i.e. 0.000012167, which guarantees an uptime of system to be  99.9987833%  which is highly acceptable uptime guarantee. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Reference](https://data-flair.training/blogs/fault-tolerance-in-apache-spark/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SPARK DEFINITIONS:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 77) Node:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 78) Worker Node:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A server that is part of the cluster and are available to run Spark jobs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 79) Master Node:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The server that coordinates the Worker nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 80) Executor:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sort of virtual machine inside a node. **One Node can have multiple Executors**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 81) Driver Node:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Node that initiates the Spark session."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 82) Driver (Executor):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Driver Node will also show up in the Executor list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 83) Driver Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark driver is a main program that declares the transformations and actions on RDDs and submits these requests to the master. This is the program where SparkContext is created.\n",
    "\n",
    "The `--driver-memory` flag controls the amount of memory to allocate for a driver, which is `1GB` by default and should be increased in case you call a collect() or take(N) action on a large RDD inside your application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 84) Executor Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The workers is where the tasks are executed `-executors`. They should have resources and network connectivity sufficient to perform transformations and actions on the RDDs defined in the main program.\n",
    "\n",
    "The `--executor-memory` flag controls the executor heap size (similarly for YARN and Slurm), the default value is 2 GB per executor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, Spark uses `60%` of the configured `executor memory (--executor-memory)` to cache RDDs. The remaining `40%` of memory is available for any objects created during task execution. In case your tasks slow down due to frequent garbage-collecting in JVM or if JVM is running out of memory, lowering this value will help reduce the memory consumption. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 85) What is the impact of `spark.cores.max` ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`spark.cores.max` is upper limit of how many cores can be allocated totally on the Hadoop cluster. If the `‘spark.executor.cores'` parameter is not set then the `'spark.cores.max'` controls the number of spark executors to be allocated.  This parameter will allow a cluster admin to restrict resource usage and there by avoid the scenario where Spark allocates too many cores. The parameter is optional when `'spark.executor.cores'` has been set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 86) no of executors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `--num-executors` defines the number of executors, which really defines the total number of applications that will be run. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The executors run on the NodeManagers (You can think of this like workers in Spark standalone). A number of Containers (includes vCPU, memory, network, disk, etc.) equal to number of executors specified will be allocated for your Spark application on YARN. Now these executor containers will be run on multiple NodeManagers and that depends on the CapacityScheduler (default scheduler in HDP)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So to sum up, total number of executors is the number of `resource containers` you specify for your application to run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `--num-executors` command-line flag or `spark.executor.instances` configuration property control the number of executors requested."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`--num-executors` is number of executors per node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 87) no of cores per executor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every Spark executor in an application has the same fixed number of cores and same fixed heap size. The number of cores can be specified with the `--executor-cores` flag when invoking spark-submit, spark-shell, and pyspark from the command line, or by setting the `spark.executor.cores` property in the `spark-defaults.conf` file or on a `SparkConf` object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, the heap size can be controlled with the `--executor-memory` flag or the `spark.executor.memory property`. The cores property controls the number of concurrent tasks an executor can run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`--executor-cores 5` means that each executor can run a maximum of five tasks at the same time. The memory property impacts the amount of data Spark can cache, as well as the maximum sizes of the shuffle data structures used for grouping, aggregations, and joins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - `--executor-memory`/`spark.executor.memory` controls the executor heap size, but JVMs can also use some memory off heap, for example for interned Strings and direct byte buffers. The value of the `spark.yarn.executor.memoryOverhead` property is added to the executor memory to determine the full memory request to YARN for each executor. It defaults to max(384, .07 * spark.executor.memory).\n",
    "\n",
    "\n",
    " - YARN may round the requested memory up a little. YARN’s `yarn.scheduler.minimum-allocation-mb` and `yarn.scheduler.increment-allocation-mb properties` control the minimum and increment request values respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following (not to scale with defaults) shows the hierarchy of memory properties in Spark and YARN:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../Resources/spark-tuning2-f1.png\" style=\"height:150px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 88) os overhead memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `Memory overhead` is the amount of `off-heap memory` allocated to `each executor`.\n",
    "\n",
    "\n",
    "- By default, memory overhead is set to either `10% of executor memory` or `384`, whichever is higher.\n",
    "\n",
    "\n",
    "- Memory overhead is used for `Java NIO direct buffers`, `thread stacks`, `shared native libraries`, or `memory mapped files`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 89) Cluster Manager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An external service for acquiring resources on the cluster (e.g. `standalone manager`, `Mesos`, `YARN`).\n",
    "\n",
    "Spark is agnostic to a cluster manager as long as it can acquire executor processes and those can communicate with each other.We are primarily interested in Yarn as the cluster manager. A spark cluster can run in either `yarn cluster` or `yarn-client` mode:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 90) yarn-client mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A driver runs on client process, Application Master is only used for requesting resources from YARN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 91) yarn-cluster mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A driver runs inside application master process, client goes away once the application is initialized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 92) Cores:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A core is a basic computation unit of CPU and a CPU may have one or more cores to perform tasks at a given time. The more cores we have, the more work we can do. In spark, this controls the number of parallel tasks an executor can run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../Resources/Nodes.png\" style=\"height:250px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 93) Steps involved in cluster mode for a Spark Job:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. From the driver code, SparkContext connects to cluster manager (standalone/Mesos/YARN).\n",
    "2. Cluster Manager allocates resources across the other applications. Any cluster manager can be used as long as the executor processes are running and they communicate with each other.\n",
    "3. Spark acquires executors on nodes in cluster. Here each application will get its own executor processes.\n",
    "4. Application code (jar/python files/python egg files) is sent to executors\n",
    "5. Tasks are sent by SparkContext to the executors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above steps, it is clear that the number of executors and their memory setting play a major role in a spark job. `Running executors with too much memory` often results in `excessive garbage collection delays`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Reference](https://spoddutur.github.io/spark-notes/distribution_of_executors_cores_and_memory_for_spark_application.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 94) garbage collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark runs on the Java Virtual Machine (JVM). Because Spark can store large amounts of data in memory, it has a major reliance on Java’s memory management and garbage collection (GC). Therefore, garbage collection (GC) can be a major issue that can affect many Spark applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Common symptoms of excessive GC in Spark are:\n",
    "\n",
    "* Application speed.\n",
    "* Executor heartbeat timeout.\n",
    "* GC overhead limit exceeded error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 95) Memory Managenent in Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Why Your Spark Applications Are Slow or Failing, Part 1: Memory Management](https://dzone.com/articles/common-reasons-your-spark-applications-are-slow-or)\n",
    "\n",
    "[Why Your Spark Apps Are Slow Or Failing, Part II: Data Skew and Garbage Collection](https://dzone.com/articles/why-your-spark-apps-are-slow-or-failing-part-ii-da)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 96) Data Skew"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In an ideal Spark application run, when Spark wants to perform a join, for example, join keys would be evenly distributed and each partition that needed processing would be nicely organized. However, real business data is rarely so neat and cooperative. We often end up with less than ideal data organization across the Spark cluster that results in degraded performance due to `data skew`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data skew is not an issue with Spark per se, rather it is a data problem. The cause of the data skew problem is the uneven distribution of the underlying data. Uneven partitioning is sometimes unavoidable in the overall data layout or the nature of the query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For joins and aggregations Spark needs to co-locate records of a single key in a single partition. Records of a key will always be in a single partition. Similarly, other key records will be distributed in other partitions. If a single partition becomes very large it will cause data skew, which will be problematic for any query engine if no special handling is done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Common symptoms of data skew are:\n",
    "\n",
    "- Frozen stages and tasks.\n",
    "- Low utilization of CPU.\n",
    "- Out of memory errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 97) Spark Internals:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Overview](https://github.com/VarunCK25/SparkInternals/blob/master/EnglishVersion/1-Overview.md)\n",
    "\n",
    "[JobLogicalPlan.](https://github.com/VarunCK25/SparkInternals/blob/master/EnglishVersion/2-JobLogicalPlan.md)\n",
    "\n",
    "[JobPhysicalPlan](https://github.com/VarunCK25/SparkInternals/blob/master/EnglishVersion/3-JobPhysicalPlan.md)\n",
    "\n",
    "[shuffleDetails](https://github.com/VarunCK25/SparkInternals/blob/master/markdown/english/4-shuffleDetails.md)\n",
    "\n",
    "[Architecture](https://github.com/VarunCK25/SparkInternals/blob/master/markdown/english/5-Architecture.md)\n",
    "\n",
    "[CacheAndCheckpoint](https://github.com/VarunCK25/SparkInternals/blob/master/markdown/english/6-CacheAndCheckpoint.md)\n",
    "\n",
    "[Broadcast](https://github.com/VarunCK25/SparkInternals/blob/master/markdown/english/7-Broadcast.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../Resources/spark_job.png\" style=\"height:500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 98) Scenario 1:\n",
    "\n",
    "<code>\n",
    "Specifications:\n",
    "\n",
    "cores - 8/per machine\n",
    "no of machines in cluster - 10\n",
    "Ram memory - 64 GB / per machine\n",
    "</code>\n",
    "\n",
    "\n",
    "**When i am running job in this cluster, job is getting completed in 1 min.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) How much time it will take to complete the same process if the cluster has 20 machines ? Justify the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) How much time it will take to complete the same process if the cluster has 100 machines ? Justify the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) How much time it will take to complete the same process if the cluster has 1000 machines ? Justify the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 99) Sceanrio 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) How your data is getting splitted across all the worker nodes in a cluster ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) What is the default partition size of the cluster ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 100) Scenario 3:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) What is the relation between process execution time and re-partition ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) What happens to the process execution time , if i put the repartition(100) whether it will increase or decrease ? Justify the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thank You !"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
