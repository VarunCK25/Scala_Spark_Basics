{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Window Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-29 11:39:38 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\r\n",
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@7ad9eda7\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "val spark = SparkSessionkSession.builder().appName(\"window_functions\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For aggregate functions, we can use the existing aggregate functions as window functions, e.g. `sum`, `avg`, `min`, `max` and `count`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class Salary\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case class Salary(depName: String, empNo: Long, salary: Long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "empsalary: org.apache.spark.sql.Dataset[Salary] = [depName: string, empNo: bigint ... 1 more field]\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val empsalary = Seq(\n",
    "  Salary(\"sales\", 1, 5000),\n",
    "  Salary(\"personnel\", 2, 3900),\n",
    "  Salary(\"sales\", 3, 4800),\n",
    "  Salary(\"sales\", 4, 4800),\n",
    "  Salary(\"personnel\", 5, 3500),\n",
    "  Salary(\"develop\", 7, 4200),\n",
    "  Salary(\"develop\", 8, 6000),\n",
    "  Salary(\"develop\", 9, 4500),\n",
    "  Salary(\"develop\", 10, 5200),\n",
    "  Salary(\"develop\", 11, 5200)).toDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res0: org.apache.spark.sql.Dataset[Salary] = [depName: string, empNo: bigint ... 1 more field]\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "empsalary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+------+\n",
      "|  depName|empNo|salary|\n",
      "+---------+-----+------+\n",
      "|    sales|    1|  5000|\n",
      "|personnel|    2|  3900|\n",
      "|    sales|    3|  4800|\n",
      "|    sales|    4|  4800|\n",
      "|personnel|    5|  3500|\n",
      "|  develop|    7|  4200|\n",
      "|  develop|    8|  6000|\n",
      "|  develop|    9|  4500|\n",
      "|  develop|   10|  5200|\n",
      "|  develop|   11|  5200|\n",
      "+---------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empsalary.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import for Window Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.expressions.Window\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.expressions.Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "byDepName: org.apache.spark.sql.expressions.WindowSpec = org.apache.spark.sql.expressions.WindowSpec@46801785\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Windows are partitions of deptName\n",
    "val byDepName = Window.partitionBy('depName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+------+-----------------+\n",
      "|  depName|empNo|salary|              avg|\n",
      "+---------+-----+------+-----------------+\n",
      "|  develop|    7|  4200|           5020.0|\n",
      "|  develop|    8|  6000|           5020.0|\n",
      "|  develop|    9|  4500|           5020.0|\n",
      "|  develop|   10|  5200|           5020.0|\n",
      "|  develop|   11|  5200|           5020.0|\n",
      "|    sales|    1|  5000|4866.666666666667|\n",
      "|    sales|    3|  4800|4866.666666666667|\n",
      "|    sales|    4|  4800|4866.666666666667|\n",
      "|personnel|    2|  3900|           3700.0|\n",
      "|personnel|    5|  3500|           3700.0|\n",
      "+---------+-----+------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empsalary.withColumn(\"avg\", avg('salary) over byDepName).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We describe a window using the convenient factory methods in Window object that create a window specification that you can further refine with **partitioning**, **ordering**, and **frame boundaries**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you describe a window you can apply window aggregate functions like **ranking** functions (e.g. `RANK`), **analytic** functions (e.g. `LAG`), and the regular aggregate functions, e.g. `sum`, `avg`, `max`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**\n",
    "\n",
    "Window functions are supported in structured queries using SQL and Column-based expressions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although similar to aggregate functions, a window function does not group rows into a single output row and retains their separate identities. A window function can access rows that are linked to the current row."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**\n",
    "\n",
    "The main difference between window aggregate functions and aggregate functions with grouping operators is that the former calculate values for every row in a window while the latter gives you at most the number of input rows, one value per group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can mark a function window by `OVER` clause after a function in SQL, e.g. `avg(revenue) OVER (…​)` or over method on a function in the Dataset API, e.g. `rank().over(…​)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Window object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Window object provides functions to define windows (as WindowSpec instances)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two families of the functions available in `Window` object that create WindowSpec instance for one or many Column instances:\n",
    "\n",
    "- partitionBy\n",
    "- orderBy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partitioning Records — `partitionBy` Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>\n",
    "    partitionBy(colName: String, colNames: String*): WindowSpec\n",
    "    partitionBy(cols: Column*): WindowSpec\n",
    "</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`partitionBy` creates an instance of `WindowSpec` with partition expression(s) defined for one or more columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tokens: org.apache.spark.sql.DataFrame = [id: int, token: string]\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val tokens = Seq((0,\"hello\"),(1,\"henry\"),(2,\"and\"),(3,\"harry\")).toDF(\"id\",\"token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id|token|\n",
      "+---+-----+\n",
      "|  0|hello|\n",
      "|  1|henry|\n",
      "|  2|  and|\n",
      "|  3|harry|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokens.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "byHTokens: org.apache.spark.sql.expressions.WindowSpec = org.apache.spark.sql.expressions.WindowSpec@38f7f9bf\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// partition records into two groups\n",
    "// * tokens starting with \"h\"\n",
    "// * others\n",
    "\n",
    "val byHTokens = Window.partitionBy('token startsWith \"h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "result: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: int, token: string ... 1 more field]\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// count the sum of ids in each group\n",
    "val result = tokens.select('*, sum('id) over byHTokens as \"sum over h tokens\").orderBy('id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----------------+\n",
      "| id|token|sum over h tokens|\n",
      "+---+-----+-----------------+\n",
      "|  0|hello|                4|\n",
      "|  1|henry|                4|\n",
      "|  2|  and|                2|\n",
      "|  3|harry|                4|\n",
      "+---+-----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordering in Windows — `orderBy` Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>\n",
    "    orderBy(colName: String, colNames: String*): WindowSpec\n",
    "    orderBy(cols: Column*): WindowSpec\n",
    "</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`orderBy` allows us to control the order of records in a window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+------+\n",
      "|  depName|empNo|salary|\n",
      "+---------+-----+------+\n",
      "|    sales|    1|  5000|\n",
      "|personnel|    2|  3900|\n",
      "|    sales|    3|  4800|\n",
      "|    sales|    4|  4800|\n",
      "|personnel|    5|  3500|\n",
      "|  develop|    7|  4200|\n",
      "|  develop|    8|  6000|\n",
      "|  develop|    9|  4500|\n",
      "|  develop|   10|  5200|\n",
      "|  develop|   11|  5200|\n",
      "+---------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empsalary.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "warning: there was one feature warning; re-run with -feature for details\n",
       "byDepnameSalaryDesc: org.apache.spark.sql.expressions.WindowSpec = org.apache.spark.sql.expressions.WindowSpec@22a9c1f3\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val byDepnameSalaryDesc = Window.partitionBy('depname).orderBy('salary desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rankByDepname: org.apache.spark.sql.Column = RANK() OVER (PARTITION BY depname ORDER BY salary DESC NULLS LAST unspecifiedframe$())\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// a numerical rank within the current row's partition for each distinct ORDER BY value\n",
    "val rankByDepname = rank().over(byDepnameSalaryDesc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+------+----+\n",
      "|  depName|empNo|salary|rank|\n",
      "+---------+-----+------+----+\n",
      "|  develop|    8|  6000|   1|\n",
      "|  develop|   10|  5200|   2|\n",
      "|  develop|   11|  5200|   2|\n",
      "|  develop|    9|  4500|   4|\n",
      "|  develop|    7|  4200|   5|\n",
      "|    sales|    1|  5000|   1|\n",
      "|    sales|    3|  4800|   2|\n",
      "|    sales|    4|  4800|   2|\n",
      "|personnel|    2|  3900|   1|\n",
      "|personnel|    5|  3500|   2|\n",
      "+---------+-----+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empsalary.select('*, rankByDepname as 'rank).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `rangeBetween` Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`\n",
    "rangeBetween(start: Long, end: Long): WindowSpec\n",
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`rangeBetween` creates a WindowSpec with the frame boundaries from `start` (inclusive) to `end` (inclusive)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**\n",
    "\n",
    "It is recommended to use Window.unboundedPreceding, Window.unboundedFollowing and Window.currentRow to describe the frame boundaries when a frame is unbounded preceding, unbounded following and at current row, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.expressions.Window\r\n",
       "import org.apache.spark.sql.expressions.WindowSpec\r\n",
       "spec: org.apache.spark.sql.expressions.WindowSpec = org.apache.spark.sql.expressions.WindowSpec@3abb9a1f\n"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.expressions.Window\n",
    "import org.apache.spark.sql.expressions.WindowSpec\n",
    "val spec: WindowSpec = Window.rangeBetween(Window.unboundedPreceding, Window.currentRow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Internally, `rangeBetween` creates a `WindowSpec` with SpecifiedWindowFrame and RangeFrame type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At its core, a window function calculates a return value for every input row of a table based on a group of rows, called the frame. Every input row can have a unique **frame** associated with it.\n",
    "\n",
    "When we define a frame you have to specify three components of a frame specification - the **start and end boundaries**, and the **type**.\n",
    "\n",
    "Types of boundaries (two positions and three offsets):\n",
    "\n",
    " - `UNBOUNDED PRECEDING` - the first row of the partition\n",
    "\n",
    " - `UNBOUNDED FOLLOWING` - the last row of the partition\n",
    "\n",
    " - `CURRENT ROW`\n",
    "\n",
    " - `<value> PRECEDING`\n",
    "\n",
    " - `<value> FOLLOWING`\n",
    "\n",
    "Offsets specify the offset from the current input row.\n",
    "\n",
    "Types of frames:\n",
    "\n",
    " - ROW - based on *physical offsets* from the position of the current input row\n",
    "\n",
    " - RANGE - based on *logical offsets* from the position of the current input row\n",
    "\n",
    "In the current implementation of WindowSpec you can use two methods to define a frame:\n",
    "\n",
    " - rowsBetween\n",
    "\n",
    " - rangeBetween"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Window Operators in SQL Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The grammar of windows operators in SQL accepts the following:\n",
    "\n",
    "1. `CLUSTER BY` or `PARTITION BY` or `DISTRIBUTE BY` for partitions,\n",
    "\n",
    "2. `ORDER BY` or `SORT BY` for sorting order,\n",
    "\n",
    "3. `RANGE`, `ROWS`, `RANGE BETWEEN`, and `ROWS BETWEEN` for window frame types,\n",
    "\n",
    "4. `UNBOUNDED PRECEDING`, `UNBOUNDED FOLLOWING`, `CURRENT ROW` for frame bounds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top N per Group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top N per Group is useful when you need to compute the first and second best-sellers in category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Question`: What are the best-selling and the second best-selling products in every category?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dataset: org.apache.spark.sql.DataFrame = [product: string, category: string ... 1 more field]\n"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dataset = Seq(\n",
    "  (\"Thin\",       \"cell phone\", 6000),\n",
    "  (\"Normal\",     \"tablet\",     1500),\n",
    "  (\"Mini\",       \"tablet\",     5500),\n",
    "  (\"Ultra thin\", \"cell phone\", 5000),\n",
    "  (\"Very thin\",  \"cell phone\", 6000),\n",
    "  (\"Big\",        \"tablet\",     2500),\n",
    "  (\"Bendable\",   \"cell phone\", 3000),\n",
    "  (\"Foldable\",   \"cell phone\", 3000),\n",
    "  (\"Pro\",        \"tablet\",     4500),\n",
    "  (\"Pro2\",       \"tablet\",     6500))\n",
    "  .toDF(\"product\", \"category\", \"revenue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-------+\n",
      "|   product|  category|revenue|\n",
      "+----------+----------+-------+\n",
      "|      Thin|cell phone|   6000|\n",
      "|    Normal|    tablet|   1500|\n",
      "|      Mini|    tablet|   5500|\n",
      "|Ultra thin|cell phone|   5000|\n",
      "| Very thin|cell phone|   6000|\n",
      "|       Big|    tablet|   2500|\n",
      "|  Bendable|cell phone|   3000|\n",
      "|  Foldable|cell phone|   3000|\n",
      "|       Pro|    tablet|   4500|\n",
      "|      Pro2|    tablet|   6500|\n",
      "+----------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-------+\n",
      "|product|category|revenue|\n",
      "+-------+--------+-------+\n",
      "| Normal|  tablet|   1500|\n",
      "|   Mini|  tablet|   5500|\n",
      "|    Big|  tablet|   2500|\n",
      "|    Pro|  tablet|   4500|\n",
      "|   Pro2|  tablet|   6500|\n",
      "+-------+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset.where('category === \"tablet\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The question boils down to ranking products in a category based on their revenue, and to pick the best selling and the second best-selling products based the ranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "overCategory: org.apache.spark.sql.expressions.WindowSpec = org.apache.spark.sql.expressions.WindowSpec@100bb85c\n"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val overCategory = Window.partitionBy('category).orderBy('revenue.desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ranked: org.apache.spark.sql.DataFrame = [product: string, category: string ... 2 more fields]\n"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val ranked = dataset.withColumn(\"dense_rank\", dense_rank.over(overCategory))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-------+----------+\n",
      "|   product|  category|revenue|dense_rank|\n",
      "+----------+----------+-------+----------+\n",
      "|      Pro2|    tablet|   6500|         1|\n",
      "|      Mini|    tablet|   5500|         2|\n",
      "|       Pro|    tablet|   4500|         3|\n",
      "|       Big|    tablet|   2500|         4|\n",
      "|    Normal|    tablet|   1500|         5|\n",
      "|      Thin|cell phone|   6000|         1|\n",
      "| Very thin|cell phone|   6000|         1|\n",
      "|Ultra thin|cell phone|   5000|         2|\n",
      "|  Bendable|cell phone|   3000|         3|\n",
      "|  Foldable|cell phone|   3000|         3|\n",
      "+----------+----------+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ranked.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Revenue Difference per Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reveDesc: org.apache.spark.sql.expressions.WindowSpec = org.apache.spark.sql.expressions.WindowSpec@2d5a69a5\n"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val reveDesc = Window.partitionBy('category).orderBy('revenue.desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reveDiff: org.apache.spark.sql.Column = (max(revenue) OVER (PARTITION BY category ORDER BY revenue DESC NULLS LAST unspecifiedframe$()) - revenue)\n"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val reveDiff = max('revenue).over(reveDesc) - 'revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-------+------------+\n",
      "|   product|  category|revenue|revenue_diff|\n",
      "+----------+----------+-------+------------+\n",
      "|      Pro2|    tablet|   6500|           0|\n",
      "|      Mini|    tablet|   5500|        1000|\n",
      "|       Pro|    tablet|   4500|        2000|\n",
      "|       Big|    tablet|   2500|        4000|\n",
      "|    Normal|    tablet|   1500|        5000|\n",
      "|      Thin|cell phone|   6000|           0|\n",
      "| Very thin|cell phone|   6000|           0|\n",
      "|Ultra thin|cell phone|   5000|        1000|\n",
      "|  Bendable|cell phone|   3000|        3000|\n",
      "|  Foldable|cell phone|   3000|        3000|\n",
      "+----------+----------+-------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset.select('*, reveDiff as 'revenue_diff).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Difference on Column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute a difference between values in rows in a column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pairs: scala.collection.immutable.IndexedSeq[(Int, Int)] = Vector((1,10), (1,20), (2,20), (2,40), (3,30), (3,60), (4,40), (4,80), (5,50), (5,100))\n"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val pairs = for {\n",
    "  x <- 1 to 5\n",
    "  y <- 1 to 2\n",
    "} yield (x, 10 * x * y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ds: org.apache.spark.sql.DataFrame = [ns: int, tens: int]\n"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val ds = pairs.toDF(\"ns\", \"tens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "| ns|tens|\n",
      "+---+----+\n",
      "|  1|  10|\n",
      "|  1|  20|\n",
      "|  2|  20|\n",
      "|  2|  40|\n",
      "|  3|  30|\n",
      "|  3|  60|\n",
      "|  4|  40|\n",
      "|  4|  80|\n",
      "|  5|  50|\n",
      "|  5| 100|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ds.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "overNs: org.apache.spark.sql.expressions.WindowSpec = org.apache.spark.sql.expressions.WindowSpec@7e315b8\n"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val overNs = Window.partitionBy('ns).orderBy('tens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "diff: org.apache.spark.sql.Column = lead(tens, 1, NULL) OVER (PARTITION BY ns ORDER BY tens ASC NULLS FIRST unspecifiedframe$())\n"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val diff = lead('tens, 1).over(overNs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----+\n",
      "| ns|tens|diff|\n",
      "+---+----+----+\n",
      "|  1|  10|  10|\n",
      "|  1|  20|null|\n",
      "|  3|  30|  30|\n",
      "|  3|  60|null|\n",
      "|  5|  50|  50|\n",
      "|  5| 100|null|\n",
      "|  4|  40|  40|\n",
      "|  4|  80|null|\n",
      "|  2|  20|  20|\n",
      "|  2|  40|null|\n",
      "+---+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ds.withColumn(\"diff\", diff - 'tens).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that `Why do Window functions fail with \"Window function X does not take a frame specification\"`?\n",
    "\n",
    "The key here is to remember that DataFrames are RDDs under the covers and hence aggregation like grouping by a key in DataFrames is RDD’s `groupBy` (or worse, `reduceByKey` or `aggregateByKey` transformations)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **running total** is the sum of all previous lines including the current one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sales: org.apache.spark.sql.DataFrame = [id: int, orderID: int ... 2 more fields]\n"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sales = Seq(\n",
    "  (0, 0, 0, 5),\n",
    "  (1, 0, 1, 3),\n",
    "  (2, 0, 2, 1),\n",
    "  (3, 1, 0, 2),\n",
    "  (4, 2, 0, 8),\n",
    "  (5, 2, 2, 8))\n",
    "  .toDF(\"id\", \"orderID\", \"prodID\", \"orderQty\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------+--------+\n",
      "| id|orderID|prodID|orderQty|\n",
      "+---+-------+------+--------+\n",
      "|  0|      0|     0|       5|\n",
      "|  1|      0|     1|       3|\n",
      "|  2|      0|     2|       1|\n",
      "|  3|      1|     0|       2|\n",
      "|  4|      2|     0|       8|\n",
      "|  5|      2|     2|       8|\n",
      "+---+-------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "orderedByID: org.apache.spark.sql.expressions.WindowSpec = org.apache.spark.sql.expressions.WindowSpec@785f135d\r\n",
       "totalQty: org.apache.spark.sql.Column = sum(orderQty) OVER (ORDER BY id ASC NULLS FIRST unspecifiedframe$()) AS `running_total`\r\n",
       "salesTotalQty: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: int, orderID: int ... 3 more fields]\n"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val orderedByID = Window.orderBy('id)\n",
    "val totalQty = sum('orderQty).over(orderedByID).as('running_total)\n",
    "val salesTotalQty = sales.select('*, totalQty).orderBy('id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-29 13:00:45 WARN  WindowExec:66 - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "+---+-------+------+--------+-------------+\n",
      "| id|orderID|prodID|orderQty|running_total|\n",
      "+---+-------+------+--------+-------------+\n",
      "|  0|      0|     0|       5|            5|\n",
      "|  1|      0|     1|       3|            8|\n",
      "|  2|      0|     2|       1|            9|\n",
      "|  3|      1|     0|       2|           11|\n",
      "|  4|      2|     0|       8|           19|\n",
      "|  5|      2|     2|       8|           27|\n",
      "+---+-------+------+--------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "salesTotalQty.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "byOrderId: org.apache.spark.sql.expressions.WindowSpec = org.apache.spark.sql.expressions.WindowSpec@6b11c331\r\n",
       "totalQtyPerOrder: org.apache.spark.sql.Column = sum(orderQty) OVER (PARTITION BY orderID ORDER BY id ASC NULLS FIRST unspecifiedframe$()) AS `running_total_per_order`\r\n",
       "salesTotalQtyPerOrder: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: int, orderID: int ... 3 more fields]\n"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val byOrderId = orderedByID.partitionBy('orderID)\n",
    "val totalQtyPerOrder = sum('orderQty).over(byOrderId).as('running_total_per_order)\n",
    "val salesTotalQtyPerOrder = sales.select('*, totalQtyPerOrder).orderBy('id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------+--------+-----------------------+\n",
      "| id|orderID|prodID|orderQty|running_total_per_order|\n",
      "+---+-------+------+--------+-----------------------+\n",
      "|  0|      0|     0|       5|                      5|\n",
      "|  1|      0|     1|       3|                      8|\n",
      "|  2|      0|     2|       1|                      9|\n",
      "|  3|      1|     0|       2|                      2|\n",
      "|  4|      2|     0|       8|                      8|\n",
      "|  5|      2|     2|       8|                     16|\n",
      "+---+-------+------+--------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "salesTotalQtyPerOrder.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Explaining\" Query Plans of Windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "warning: there was one feature warning; re-run with -feature for details\n",
       "byDepnameSalaryDesc: org.apache.spark.sql.expressions.WindowSpec = org.apache.spark.sql.expressions.WindowSpec@3ea89de6\n"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val byDepnameSalaryDesc = Window.partitionBy('depname).orderBy('salary desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculate rank of row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rankByDepname: org.apache.spark.sql.Column = RANK() OVER (PARTITION BY depname ORDER BY salary DESC NULLS LAST unspecifiedframe$())\n"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rankByDepname = rank().over(byDepnameSalaryDesc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+------+\n",
      "|  depName|empNo|salary|\n",
      "+---------+-----+------+\n",
      "|    sales|    1|  5000|\n",
      "|personnel|    2|  3900|\n",
      "|    sales|    3|  4800|\n",
      "|    sales|    4|  4800|\n",
      "|personnel|    5|  3500|\n",
      "|  develop|    7|  4200|\n",
      "|  develop|    8|  6000|\n",
      "|  develop|    9|  4500|\n",
      "|  develop|   10|  5200|\n",
      "|  develop|   11|  5200|\n",
      "+---------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empsalary.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Project [*, rank() windowspecdefinition('depname, 'salary DESC NULLS LAST, unspecifiedframe$()) AS rank#1626]\n",
      "+- AnalysisBarrier\n",
      "      +- LocalRelation [depName#3, empNo#4L, salary#5L]\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "depName: string, empNo: bigint, salary: bigint, rank: int\n",
      "Project [depName#3, empNo#4L, salary#5L, rank#1626]\n",
      "+- Project [depName#3, empNo#4L, salary#5L, rank#1626, rank#1626]\n",
      "   +- Window [rank(salary#5L) windowspecdefinition(depname#3, salary#5L DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rank#1626], [depname#3], [salary#5L DESC NULLS LAST]\n",
      "      +- Project [depName#3, empNo#4L, salary#5L]\n",
      "         +- LocalRelation [depName#3, empNo#4L, salary#5L]\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Window [rank(salary#5L) windowspecdefinition(depname#3, salary#5L DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rank#1626], [depname#3], [salary#5L DESC NULLS LAST]\n",
      "+- LocalRelation [depName#3, empNo#4L, salary#5L]\n",
      "\n",
      "== Physical Plan ==\n",
      "Window [rank(salary#5L) windowspecdefinition(depname#3, salary#5L DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rank#1626], [depname#3], [salary#5L DESC NULLS LAST]\n",
      "+- *(1) Sort [depname#3 ASC NULLS FIRST, salary#5L DESC NULLS LAST], false, 0\n",
      "   +- Exchange hashpartitioning(depname#3, 200)\n",
      "      +- LocalTableScan [depName#3, empNo#4L, salary#5L]\n"
     ]
    }
   ],
   "source": [
    "empsalary.select('*, rankByDepname as 'rank).explain(extended = true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `lag` Window Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>\n",
    "    lag(e: Column, offset: Int): Column\n",
    "    lag(columnName: String, offset: Int): Column\n",
    "    lag(columnName: String, offset: Int, defaultValue: Any): Column\n",
    "    lag(e: Column, offset: Int, defaultValue: Any): Column\n",
    "</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`lag` returns the value in `e / columnName` column that is `offset` records before the current record."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`lag` returns `null` value if the number of records in a window partition is less than `offset` or `defaultValue`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "buckets: org.apache.spark.sql.DataFrame = [id: bigint, bucket: bigint]\n"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val buckets = spark.range(9).withColumn(\"bucket\", 'id % 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|bucket|\n",
      "+---+------+\n",
      "|  0|     0|\n",
      "|  1|     1|\n",
      "|  2|     2|\n",
      "|  3|     0|\n",
      "|  4|     1|\n",
      "|  5|     2|\n",
      "|  6|     0|\n",
      "|  7|     1|\n",
      "|  8|     2|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "buckets.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dataset: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: bigint, bucket: bigint]\n"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Make duplicates\n",
    "val dataset = buckets.union(buckets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|bucket|\n",
      "+---+------+\n",
      "|  0|     0|\n",
      "|  1|     1|\n",
      "|  2|     2|\n",
      "|  3|     0|\n",
      "|  4|     1|\n",
      "|  5|     2|\n",
      "|  6|     0|\n",
      "|  7|     1|\n",
      "|  8|     2|\n",
      "|  0|     0|\n",
      "|  1|     1|\n",
      "|  2|     2|\n",
      "|  3|     0|\n",
      "|  4|     1|\n",
      "|  5|     2|\n",
      "|  6|     0|\n",
      "|  7|     1|\n",
      "|  8|     2|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "windowSpec: org.apache.spark.sql.expressions.WindowSpec = org.apache.spark.sql.expressions.WindowSpec@4d9507db\n"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val windowSpec = Window.partitionBy('bucket).orderBy('id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----+\n",
      "| id|bucket| lag|\n",
      "+---+------+----+\n",
      "|  0|     0|null|\n",
      "|  0|     0|   0|\n",
      "|  3|     0|   0|\n",
      "|  3|     0|   3|\n",
      "|  6|     0|   3|\n",
      "|  6|     0|   6|\n",
      "|  1|     1|null|\n",
      "|  1|     1|   1|\n",
      "|  4|     1|   1|\n",
      "|  4|     1|   4|\n",
      "|  7|     1|   4|\n",
      "|  7|     1|   7|\n",
      "|  2|     2|null|\n",
      "|  2|     2|   2|\n",
      "|  5|     2|   2|\n",
      "|  5|     2|   5|\n",
      "|  8|     2|   5|\n",
      "|  8|     2|   8|\n",
      "+---+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset.withColumn(\"lag\", lag('id, 1) over windowSpec).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----+\n",
      "| id|bucket| lag|\n",
      "+---+------+----+\n",
      "|  0|     0|null|\n",
      "|  0|     0|null|\n",
      "|  3|     0|   0|\n",
      "|  3|     0|   0|\n",
      "|  6|     0|   3|\n",
      "|  6|     0|   3|\n",
      "|  1|     1|null|\n",
      "|  1|     1|null|\n",
      "|  4|     1|   1|\n",
      "|  4|     1|   1|\n",
      "|  7|     1|   4|\n",
      "|  7|     1|   4|\n",
      "|  2|     2|null|\n",
      "|  2|     2|null|\n",
      "|  5|     2|   2|\n",
      "|  5|     2|   2|\n",
      "|  8|     2|   5|\n",
      "|  8|     2|   5|\n",
      "+---+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset.withColumn(\"lag\", lag('id, 2, \"<default_value>\") over windowSpec).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Caution**\n",
    "\n",
    "It looks like `lag` with a default value has a bug — the default value’s not used at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `lead` Window Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>\n",
    "    lead(columnName: String, offset: Int): Column\n",
    "    lead(e: Column, offset: Int): Column\n",
    "    lead(columnName: String, offset: Int, defaultValue: Any): Column\n",
    "    lead(e: Column, offset: Int, defaultValue: Any): Column\n",
    "</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`lead` returns the value that is `offset` records after the current records, and `defaultValue` if there is less than `offset` records after the current record. `lag` returns `null` value if the number of records in a window partition is less than `offset` or `defaultValue`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----+\n",
      "| id|bucket|lead|\n",
      "+---+------+----+\n",
      "|  0|     0|   0|\n",
      "|  0|     0|   3|\n",
      "|  3|     0|   3|\n",
      "|  3|     0|   6|\n",
      "|  6|     0|   6|\n",
      "|  6|     0|null|\n",
      "|  1|     1|   1|\n",
      "|  1|     1|   4|\n",
      "|  4|     1|   4|\n",
      "|  4|     1|   7|\n",
      "|  7|     1|   7|\n",
      "|  7|     1|null|\n",
      "|  2|     2|   2|\n",
      "|  2|     2|   5|\n",
      "|  5|     2|   5|\n",
      "|  5|     2|   8|\n",
      "|  8|     2|   8|\n",
      "|  8|     2|null|\n",
      "+---+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset.withColumn(\"lead\", lead('id, 1) over windowSpec).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----+\n",
      "| id|bucket|lead|\n",
      "+---+------+----+\n",
      "|  0|     0|   3|\n",
      "|  0|     0|   3|\n",
      "|  3|     0|   6|\n",
      "|  3|     0|   6|\n",
      "|  6|     0|null|\n",
      "|  6|     0|null|\n",
      "|  1|     1|   4|\n",
      "|  1|     1|   4|\n",
      "|  4|     1|   7|\n",
      "|  4|     1|   7|\n",
      "|  7|     1|null|\n",
      "|  7|     1|null|\n",
      "|  2|     2|   5|\n",
      "|  2|     2|   5|\n",
      "|  5|     2|   8|\n",
      "|  5|     2|   8|\n",
      "|  8|     2|null|\n",
      "|  8|     2|null|\n",
      "+---+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset.withColumn(\"lead\", lead('id, 2, \"<default_value>\") over windowSpec).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Caution**\n",
    "\n",
    "It looks like `lead` with a default value has a bug — the default value’s not used at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cumulative Distribution of Records Across Window Partitions — `cume_dist` Window Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`\n",
    "cume_dist(): Column\n",
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`cume_dist` computes the cumulative distribution of the records in window partitions. This is equivalent to SQL’s `CUME_DIST` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------------------+\n",
      "| id|bucket|         cume_dist|\n",
      "+---+------+------------------+\n",
      "|  0|     0|0.3333333333333333|\n",
      "|  0|     0|0.3333333333333333|\n",
      "|  3|     0|0.6666666666666666|\n",
      "|  3|     0|0.6666666666666666|\n",
      "|  6|     0|               1.0|\n",
      "|  6|     0|               1.0|\n",
      "|  1|     1|0.3333333333333333|\n",
      "|  1|     1|0.3333333333333333|\n",
      "|  4|     1|0.6666666666666666|\n",
      "|  4|     1|0.6666666666666666|\n",
      "|  7|     1|               1.0|\n",
      "|  7|     1|               1.0|\n",
      "|  2|     2|0.3333333333333333|\n",
      "|  2|     2|0.3333333333333333|\n",
      "|  5|     2|0.6666666666666666|\n",
      "|  5|     2|0.6666666666666666|\n",
      "|  8|     2|               1.0|\n",
      "|  8|     2|               1.0|\n",
      "+---+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset.withColumn(\"cume_dist\", cume_dist over windowSpec).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential numbering per window partition — `row_number` Window Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`\n",
    "row_number(): Column\n",
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`row_number` returns a sequential number starting at `1` within a window partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------+\n",
      "| id|bucket|row_number|\n",
      "+---+------+----------+\n",
      "|  0|     0|         1|\n",
      "|  0|     0|         2|\n",
      "|  3|     0|         3|\n",
      "|  3|     0|         4|\n",
      "|  6|     0|         5|\n",
      "|  6|     0|         6|\n",
      "|  1|     1|         1|\n",
      "|  1|     1|         2|\n",
      "|  4|     1|         3|\n",
      "|  4|     1|         4|\n",
      "|  7|     1|         5|\n",
      "|  7|     1|         6|\n",
      "|  2|     2|         1|\n",
      "|  2|     2|         2|\n",
      "|  5|     2|         3|\n",
      "|  5|     2|         4|\n",
      "|  8|     2|         5|\n",
      "|  8|     2|         6|\n",
      "+---+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset.withColumn(\"row_number\", row_number() over windowSpec).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `ntile` Window Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`\n",
    "ntile(n: Int): Column\n",
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ntile` computes the ntile group id (from `1` to `n` inclusive) in an ordered window partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dataset: org.apache.spark.sql.DataFrame = [id: bigint, bucket: bigint]\n"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dataset = spark.range(7).select('*, 'id % 3 as \"bucket\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|bucket|\n",
      "+---+------+\n",
      "|  0|     0|\n",
      "|  1|     1|\n",
      "|  2|     2|\n",
      "|  3|     0|\n",
      "|  4|     1|\n",
      "|  5|     2|\n",
      "|  6|     0|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "byBuckets: org.apache.spark.sql.expressions.WindowSpec = org.apache.spark.sql.expressions.WindowSpec@acf500b\n"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val byBuckets = Window.partitionBy('bucket).orderBy('id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+\n",
      "| id|bucket|ntile|\n",
      "+---+------+-----+\n",
      "|  0|     0|    1|\n",
      "|  3|     0|    2|\n",
      "|  6|     0|    3|\n",
      "|  1|     1|    1|\n",
      "|  4|     1|    2|\n",
      "|  2|     2|    1|\n",
      "|  5|     2|    2|\n",
      "+---+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset.select('*, ntile(3) over byBuckets as \"ntile\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ranking Records per Window Partition — `rank` Window Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`\n",
    "rank(): Column\n",
    "dense_rank(): Column\n",
    "percent_rank(): Column\n",
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`rank` functions assign the sequential rank of each distinct value per window partition. They are equivalent to `RANK`, `DENSE_RANK` and `PERCENT_RANK` functions in the good ol' SQL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`\n",
    "rank(): Column\n",
    "dense_rank(): Column\n",
    "percent_rank(): Column\n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dataset: org.apache.spark.sql.DataFrame = [id: bigint, bucket: bigint]\n"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dataset = spark.range(9).withColumn(\"bucket\", 'id % 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|bucket|\n",
      "+---+------+\n",
      "|  0|     0|\n",
      "|  1|     1|\n",
      "|  2|     2|\n",
      "|  3|     0|\n",
      "|  4|     1|\n",
      "|  5|     2|\n",
      "|  6|     0|\n",
      "|  7|     1|\n",
      "|  8|     2|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "byBucket: org.apache.spark.sql.expressions.WindowSpec = org.apache.spark.sql.expressions.WindowSpec@11675596\n"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val byBucket = Window.partitionBy('bucket).orderBy('id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----+\n",
      "| id|bucket|rank|\n",
      "+---+------+----+\n",
      "|  0|     0|   1|\n",
      "|  3|     0|   2|\n",
      "|  6|     0|   3|\n",
      "|  1|     1|   1|\n",
      "|  4|     1|   2|\n",
      "|  7|     1|   3|\n",
      "|  2|     2|   1|\n",
      "|  5|     2|   2|\n",
      "|  8|     2|   3|\n",
      "+---+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset.withColumn(\"rank\", rank over byBucket).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dense_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------+\n",
      "| id|bucket|dense_rank|\n",
      "+---+------+----------+\n",
      "|  0|     0|         1|\n",
      "|  3|     0|         2|\n",
      "|  6|     0|         3|\n",
      "|  1|     1|         1|\n",
      "|  4|     1|         2|\n",
      "|  7|     1|         3|\n",
      "|  2|     2|         1|\n",
      "|  5|     2|         2|\n",
      "|  8|     2|         3|\n",
      "+---+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset.withColumn(\"dense_rank\", dense_rank over byBucket).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### percent_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------------+\n",
      "| id|bucket|percent_rank|\n",
      "+---+------+------------+\n",
      "|  0|     0|         0.0|\n",
      "|  3|     0|         0.5|\n",
      "|  6|     0|         1.0|\n",
      "|  1|     1|         0.0|\n",
      "|  4|     1|         0.5|\n",
      "|  7|     1|         1.0|\n",
      "|  2|     2|         0.0|\n",
      "|  5|     2|         0.5|\n",
      "|  8|     2|         1.0|\n",
      "+---+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset.withColumn(\"percent_rank\", percent_rank over byBucket).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`rank` function assigns the same rank for duplicate rows with a gap in the sequence (similarly to Olympic medal places). `dense_rank` is like `rank` for duplicate rows but compacts the ranks and removes the gaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----+\n",
      "| id|bucket|rank|\n",
      "+---+------+----+\n",
      "|  0|     0|   1|\n",
      "|  0|     0|   1|\n",
      "|  3|     0|   3|\n",
      "|  3|     0|   3|\n",
      "|  6|     0|   5|\n",
      "|  6|     0|   5|\n",
      "|  1|     1|   1|\n",
      "|  1|     1|   1|\n",
      "|  4|     1|   3|\n",
      "|  4|     1|   3|\n",
      "|  7|     1|   5|\n",
      "|  7|     1|   5|\n",
      "|  2|     2|   1|\n",
      "|  2|     2|   1|\n",
      "|  5|     2|   3|\n",
      "|  5|     2|   3|\n",
      "|  8|     2|   5|\n",
      "|  8|     2|   5|\n",
      "+---+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// rank function with duplicates\n",
    "// Note the missing/sparse ranks, i.e. 2 and 4\n",
    "dataset.union(dataset).withColumn(\"rank\", rank over byBucket).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------+\n",
      "| id|bucket|dense_rank|\n",
      "+---+------+----------+\n",
      "|  0|     0|         1|\n",
      "|  0|     0|         1|\n",
      "|  3|     0|         2|\n",
      "|  3|     0|         2|\n",
      "|  6|     0|         3|\n",
      "|  6|     0|         3|\n",
      "|  1|     1|         1|\n",
      "|  1|     1|         1|\n",
      "|  4|     1|         2|\n",
      "|  4|     1|         2|\n",
      "|  7|     1|         3|\n",
      "|  7|     1|         3|\n",
      "|  2|     2|         1|\n",
      "|  2|     2|         1|\n",
      "|  5|     2|         2|\n",
      "|  5|     2|         2|\n",
      "|  8|     2|         3|\n",
      "|  8|     2|         3|\n",
      "+---+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// dense_rank function with duplicates\n",
    "// Note that the missing ranks are now filled in\n",
    "dataset.union(dataset).withColumn(\"dense_rank\", dense_rank over byBucket).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------------+\n",
      "| id|bucket|percent_rank|\n",
      "+---+------+------------+\n",
      "|  0|     0|         0.0|\n",
      "|  0|     0|         0.0|\n",
      "|  3|     0|         0.4|\n",
      "|  3|     0|         0.4|\n",
      "|  6|     0|         0.8|\n",
      "|  6|     0|         0.8|\n",
      "|  1|     1|         0.0|\n",
      "|  1|     1|         0.0|\n",
      "|  4|     1|         0.4|\n",
      "|  4|     1|         0.4|\n",
      "|  7|     1|         0.8|\n",
      "|  7|     1|         0.8|\n",
      "|  2|     2|         0.0|\n",
      "|  2|     2|         0.0|\n",
      "|  5|     2|         0.4|\n",
      "|  5|     2|         0.4|\n",
      "|  8|     2|         0.8|\n",
      "|  8|     2|         0.8|\n",
      "+---+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// percent_rank function with duplicates\n",
    "dataset.union(dataset).withColumn(\"percent_rank\", percent_rank over byBucket).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Closing Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
