{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://Varun-CK:4040\n",
       "SparkContext available as 'sc' (version = 2.3.0, master = local[*], app id = local-1579453089921)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-19 22:28:24 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@4b59fc05\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spark = SparkSession.builder().getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dataframe from tags file question_tags_10K.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dfTags: org.apache.spark.sql.DataFrame = [id: int, tag: string]\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dfTags = spark\n",
    "    .read\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .csv(\"..\\\\Resources\\\\question_tags_10K.csv\")\n",
    "    .toDF(\"id\", \"tag\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+\n",
      "| id|            tag|\n",
      "+---+---------------+\n",
      "|  1|           data|\n",
      "|  4|             c#|\n",
      "|  4|       winforms|\n",
      "|  4|type-conversion|\n",
      "|  4|        decimal|\n",
      "+---+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfTags.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dataframe from questions file questions_10K.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dfQuestionsCSV: org.apache.spark.sql.DataFrame = [id: int, creation_date: timestamp ... 5 more fields]\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dfQuestionsCSV = spark\n",
    "    .read\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .option(\"dateFormat\",\"yyyy-MM-dd HH:mm:ss\")\n",
    "    .csv(\"..\\\\Resources\\\\questions_10K.csv\")\n",
    "    .toDF(\"id\", \"creation_date\", \"closed_date\", \"deletion_date\", \"score\", \"owner_userid\", \"answer_count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+--------------------+--------------------+-----+------------+------------+\n",
      "| id|      creation_date|         closed_date|       deletion_date|score|owner_userid|answer_count|\n",
      "+---+-------------------+--------------------+--------------------+-----+------------+------------+\n",
      "|  1|2008-08-01 02:56:37|                  NA|2011-03-28T00:53:47Z|    1|          NA|           0|\n",
      "|  4|2008-08-01 03:12:52|                  NA|                  NA|  472|           8|          13|\n",
      "|  6|2008-08-01 03:38:08|                  NA|                  NA|  210|           9|           5|\n",
      "|  8|2008-08-01 05:03:19|2013-06-03T04:00:25Z|2015-02-11T08:26:40Z|   42|          NA|           8|\n",
      "|  9|2008-08-01 05:10:59|                  NA|                  NA| 1452|           1|          58|\n",
      "+---+-------------------+--------------------+--------------------+-----+------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfQuestionsCSV.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cast columns to data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dfQuestions: org.apache.spark.sql.DataFrame = [id: int, creation_date: timestamp ... 5 more fields]\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dfQuestions = dfQuestionsCSV.select(\n",
    "    dfQuestionsCSV.col(\"id\").cast(\"integer\"),\n",
    "    dfQuestionsCSV.col(\"creation_date\").cast(\"timestamp\"),\n",
    "    dfQuestionsCSV.col(\"closed_date\").cast(\"timestamp\"),\n",
    "    dfQuestionsCSV.col(\"deletion_date\").cast(\"date\"),\n",
    "    dfQuestionsCSV.col(\"score\").cast(\"integer\"),\n",
    "    dfQuestionsCSV.col(\"owner_userid\").cast(\"integer\"),\n",
    "    dfQuestionsCSV.col(\"answer_count\").cast(\"integer\")\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- creation_date: timestamp (nullable = true)\n",
      " |-- closed_date: timestamp (nullable = true)\n",
      " |-- deletion_date: date (nullable = true)\n",
      " |-- score: integer (nullable = true)\n",
      " |-- owner_userid: integer (nullable = true)\n",
      " |-- answer_count: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfQuestions.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports for functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.{avg, max, min, mean, sum}\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.{avg,max,min,mean,sum}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|       avg(score)|\n",
      "+-----------------+\n",
      "|36.14631463146315|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfQuestions.select(avg(\"score\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|max(score)|\n",
      "+----------+\n",
      "|      4443|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfQuestions.select(max(\"score\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|min(score)|\n",
      "+----------+\n",
      "|       -27|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfQuestions.select(min(\"score\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|       avg(score)|\n",
      "+-----------------+\n",
      "|36.14631463146315|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfQuestions.select(mean(\"score\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|sum(score)|\n",
      "+----------+\n",
      "|    361427|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfQuestions.select(sum(\"score\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|count(score)|\n",
      "+------------+\n",
      "|        9999|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfQuestions.select(count(\"score\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group by with statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+-----------------+\n",
      "|owner_userid|avg(score)|max(answer_count)|\n",
      "+------------+----------+-----------------+\n",
      "|         268|      26.0|                1|\n",
      "|         136|      57.6|                9|\n",
      "|         123|      20.0|                3|\n",
      "+------------+----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfQuestions\n",
    "    .filter(\"id > 400 and id < 450\")\n",
    "    .filter(\"owner_userid is not null\")\n",
    "    .join(dfTags, dfQuestions.col(\"id\").equalTo(dfTags(\"id\")))\n",
    "    .groupBy(dfQuestions.col(\"owner_userid\"))\n",
    "    .agg(avg(\"score\"), max(\"answer_count\"))\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame Statistics using describe() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dfQuestionsStatistics: org.apache.spark.sql.DataFrame = [summary: string, id: string ... 3 more fields]\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dfQuestionsStatistics = dfQuestions.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------------+-----------------+------------------+\n",
      "|summary|               id|             score|     owner_userid|      answer_count|\n",
      "+-------+-----------------+------------------+-----------------+------------------+\n",
      "|  count|             9999|              9999|             7388|              9922|\n",
      "|   mean|33929.17081708171| 36.14631463146315|47389.99472116947|6.6232614392259626|\n",
      "| stddev|19110.09560532429|160.48316753972045|280943.1070344427| 9.069109116851138|\n",
      "|    min|                1|               -27|                1|                -5|\n",
      "|    max|            66037|              4443|          3431280|               316|\n",
      "+-------+-----------------+------------------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfQuestionsStatistics.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "correlation: Double = 0.3699847903294707\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val correlation = dfQuestions.stat.corr(\"score\", \"answer_count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correlation between column score and answer_count = 0.3699847903294707\n"
     ]
    }
   ],
   "source": [
    "println(s\"correlation between column score and answer_count = $correlation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Covariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "covariance: Double = 537.513381444165\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val covariance = dfQuestions.stat.cov(\"score\", \"answer_count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "covariance between column score and answer_count = 537.513381444165\n"
     ]
    }
   ],
   "source": [
    "println(s\"covariance between column score and answer_count = $covariance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequent Items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dfFrequentScore: org.apache.spark.sql.DataFrame = [answer_count_freqItems: array<int>]\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dfFrequentScore = dfQuestions.stat.freqItems(Seq(\"answer_count\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|answer_count_freqItems                                                                                                                                                                                                                                                                                                                                 |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|[23, 131, 77, 86, 32, 50, 41, 53, 35, 17, 8, 44, 26, -1, 89, 80, 71, 11, 56, 29, 38, 47, 20, 2, 65, 316, 5, -4, 14, 214, 46, 100, 55, 73, 67, 58, 40, 49, 13, 4, 31, 22, 103, -5, 97, 16, 7, -2, 52, 43, 25, 34, 61, 10, 37, 1, 28, 19, 129, 87, 114, 78, 69, 63, 99, 45, 54, 27, 36, 18, 9, 48, 21, 57, 3, 12, 30, 39, 15, 42, 24, 6, 33, -3,, 0, 296]|\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfFrequentScore.show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crosstab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dfScoreByUserid: org.apache.spark.sql.DataFrame = [score_owner_userid: string, 1: bigint ... 9 more fields]\n"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dfScoreByUserid = dfQuestions\n",
    "    .filter(\"owner_userid > 0 and owner_userid < 20\")\n",
    "    .stat\n",
    "    .crosstab(\"score\", \"owner_userid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---+---+---+---+---+---+---+---+---+---+\n",
      "|score_owner_userid|  1| 11| 13| 17|  2|  3|  4|  5|  8|  9|\n",
      "+------------------+---+---+---+---+---+---+---+---+---+---+\n",
      "|                56|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|\n",
      "|               472|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|\n",
      "|                14|  0|  0|  0|  1|  0|  0|  0|  1|  0|  0|\n",
      "|                20|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|\n",
      "|               179|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|\n",
      "+------------------+---+---+---+---+---+---+---+---+---+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfScoreByUserid.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stratified sampling using sampleBy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### find all rows where answer_count in (5, 10, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dfQuestionsByAnswerCount: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: int, creation_date: timestamp ... 5 more fields]\n"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dfQuestionsByAnswerCount = dfQuestions\n",
    "    .filter(\"owner_userid > 0\")\n",
    "    .filter(\"answer_count in (5, 10, 20)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res17: org.apache.spark.sql.Row = [6,2008-08-01 03:38:08.0,null,null,210,9,5]\n"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfQuestionsByAnswerCount.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### count how many rows match answer_count in (5, 10, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|answer_count|count|\n",
      "+------------+-----+\n",
      "|          20|   34|\n",
      "|           5|  811|\n",
      "|          10|  272|\n",
      "+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfQuestionsByAnswerCount\n",
    "    .groupBy(\"answer_count\")\n",
    "    .count()\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a fraction map where we are only interested:**\n",
    "- 50% of the rows that have answer_count = 5\n",
    "- 10% of the rows that have answer_count = 10\n",
    "- 100% of the rows that have answer_count = 20\n",
    "\n",
    "*Note also that fractions should be in the range [0, 1]*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fractionKeyMap: scala.collection.immutable.Map[Int,Double] = Map(5 -> 0.5, 10 -> 0.1, 20 -> 1.0)\n"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val fractionKeyMap = Map(5 -> 0.5, 10 -> 0.1, 20 -> 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stratified sample using the fractionKeyMap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-19 22:56:19 WARN  BaseSessionStateBuilder$$anon$2:66 - Max iterations (100) reached for batch Operator Optimization before Inferring Filters\n",
      "2020-01-19 22:56:20 WARN  BaseSessionStateBuilder$$anon$2:66 - Max iterations (100) reached for batch Operator Optimization after Inferring Filters\n",
      "+------------+-----+\n",
      "|answer_count|count|\n",
      "+------------+-----+\n",
      "|          20|   34|\n",
      "|           5|  400|\n",
      "|          10|   26|\n",
      "+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfQuestionsByAnswerCount\n",
    "    .stat\n",
    "    .sampleBy(\"answer_count\", fractionKeyMap, seed=7L)\n",
    "    .groupBy(\"answer_count\")\n",
    "    .count()\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that changing the random seed will modify our sampling outcome. As an example, let's change the random seed to 37."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-19 22:58:32 WARN  BaseSessionStateBuilder$$anon$2:66 - Max iterations (100) reached for batch Operator Optimization before Inferring Filters\n",
      "2020-01-19 22:58:32 WARN  BaseSessionStateBuilder$$anon$2:66 - Max iterations (100) reached for batch Operator Optimization after Inferring Filters\n",
      "+------------+-----+\n",
      "|answer_count|count|\n",
      "+------------+-----+\n",
      "|          20|   34|\n",
      "|           5|  388|\n",
      "|          10|   25|\n",
      "+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfQuestionsByAnswerCount\n",
    "    .stat\n",
    "    .sampleBy(\"answer_count\", fractionKeyMap, seed=37L)\n",
    "    .groupBy(\"answer_count\")\n",
    "    .count()\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approximate Quantile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "quantiles: Array[Double] = Array(-27.0, 2.0, 4443.0)\n"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val quantiles = dfQuestions\n",
    "    .stat\n",
    "    .approxQuantile(\"score\", Array(0, 0.5, 1), 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qauntiles segments = WrappedArray(-27.0, 2.0, 4443.0)\n"
     ]
    }
   ],
   "source": [
    "println(s\"Qauntiles segments = ${quantiles.toSeq}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can verify the quantiles statistics above using Spark SQL as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfQuestions.createOrReplaceTempView(\"so_questions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------------------------------------+----------+\n",
      "|min(score)|percentile_approx(score, CAST(0.25 AS DOUBLE), 10000)|max(score)|\n",
      "+----------+-----------------------------------------------------+----------+\n",
      "|       -27|                                                    2|      4443|\n",
      "+----------+-----------------------------------------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select min(score), percentile_approx(score, 0.25), max(score) from so_questions\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bloom Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tagsBloomFilter: org.apache.spark.util.sketch.BloomFilter = org.apache.spark.util.sketch.BloomFilterImpl@809c4023\n"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val tagsBloomFilter = dfTags.stat.bloomFilter(colName=\"tag\", expectedNumItems=1000L, fpp=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bloom filter contains java tag = true\n"
     ]
    }
   ],
   "source": [
    "println(s\"bloom filter contains java tag = ${tagsBloomFilter.mightContain(\"java\")}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bloom filter contains some unknown tag = false\n"
     ]
    }
   ],
   "source": [
    "println(s\"bloom filter contains some unknown tag = ${tagsBloomFilter.mightContain(\"unknown tag\")}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Min Sketch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cmsTag: org.apache.spark.util.sketch.CountMinSketch = org.apache.spark.util.sketch.CountMinSketchImpl@431a88ed\n"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val cmsTag = dfTags.stat.countMinSketch(colName=\"tag\", eps=0.1, confidence=0.9, seed=37)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "estimatedFrequency: Long = 513\n"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val estimatedFrequency = cmsTag.estimateCount(\"java\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated frequency for tag java = 513\n"
     ]
    }
   ],
   "source": [
    "println(s\"Estimated frequency for tag java = $estimatedFrequency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling With Replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dfTagsSample: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: int, tag: string]\n"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dfTagsSample = dfTags.sample(withReplacement=true, fraction=0.2, seed=37L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in sample dfTagsSample = 1948\n"
     ]
    }
   ],
   "source": [
    "println(s\"Number of rows in sample dfTagsSample = ${dfTagsSample.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in dfTags = 9999\n"
     ]
    }
   ],
   "source": [
    "println(s\"Number of rows in dfTags = ${dfTags.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Closing Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
