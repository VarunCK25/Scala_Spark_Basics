{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://Varun-CK:4040\n",
       "SparkContext available as 'sc' (version = 2.3.0, master = local[*], app id = local-1579452082591)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.{SparkSession, Dataset}\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.{SparkSession,Dataset}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-19 22:11:37 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@5f1ca5fc\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dfTags: org.apache.spark.sql.DataFrame = [id: int, tag: string]\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dfTags = spark\n",
    "    .read\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .csv(\"..\\\\Resources\\\\question_tags_10K.csv\")\n",
    "    .toDF(\"id\", \"tag\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+\n",
      "| id|            tag|\n",
      "+---+---------------+\n",
      "|  1|           data|\n",
      "|  4|             c#|\n",
      "|  4|       winforms|\n",
      "|  4|type-conversion|\n",
      "|  4|        decimal|\n",
      "+---+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfTags.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dfQuestionsCSV: org.apache.spark.sql.DataFrame = [id: string, creation_date: string ... 5 more fields]\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dfQuestionsCSV = spark\n",
    "    .read\n",
    "    .option(\"header\", false)\n",
    "    .option(\"inferSchema\", true)\n",
    "    .option(\"dateFormat\",\"yyyy-MM-dd HH:mm:ss\")\n",
    "    .csv(\"..\\\\resources\\\\questions_10K.csv\")\n",
    "    .toDF(\"id\", \"creation_date\", \"closed_date\", \"deletion_date\", \"score\", \"owner_userid\", \"answer_count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+--------------------+-----+------------+------------+\n",
      "| id|       creation_date|         closed_date|       deletion_date|score|owner_userid|answer_count|\n",
      "+---+--------------------+--------------------+--------------------+-----+------------+------------+\n",
      "| Id|        CreationDate|          ClosedDate|        DeletionDate|Score| OwnerUserId| AnswerCount|\n",
      "|  1|2008-07-31T21:26:37Z|                  NA|2011-03-28T00:53:47Z|    1|          NA|           0|\n",
      "|  4|2008-07-31T21:42:52Z|                  NA|                  NA|  472|           8|          13|\n",
      "|  6|2008-07-31T22:08:08Z|                  NA|                  NA|  210|           9|           5|\n",
      "|  8|2008-07-31T23:33:19Z|2013-06-03T04:00:25Z|2015-02-11T08:26:40Z|   42|          NA|           8|\n",
      "+---+--------------------+--------------------+--------------------+-----+------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfQuestionsCSV.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dfQuestions: org.apache.spark.sql.DataFrame = [owner_userid: string, tag: string ... 2 more fields]\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dfQuestions = dfQuestionsCSV\n",
    "    .filter(\"score > 400 and score < 410\")\n",
    "    .join(dfTags, \"id\")\n",
    "    .select(\"owner_userid\", \"tag\", \"creation_date\", \"score\")\n",
    "    .toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+--------------------+-----+\n",
      "|owner_userid|      tag|       creation_date|score|\n",
      "+------------+---------+--------------------+-----+\n",
      "|         131|   xdebug|2008-08-03T23:18:21Z|  405|\n",
      "|         131| phpstorm|2008-08-03T23:18:21Z|  405|\n",
      "|         131|debugging|2008-08-03T23:18:21Z|  405|\n",
      "|         131|  eclipse|2008-08-03T23:18:21Z|  405|\n",
      "|         131|      php|2008-08-03T23:18:21Z|  405|\n",
      "+------------+---------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfQuestions.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert DataFrame row to Scala case class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class Tag\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case class Tag(id:Int, tag:String)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import spark.implicits._\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dfTagsOfTag: org.apache.spark.sql.Dataset[Tag] = [id: int, tag: string]\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dfTagsOfTag: Dataset[Tag] = dfTags.as[Tag]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "| id|     tag|\n",
      "+---+--------+\n",
      "|  1|    data|\n",
      "|  4|      c#|\n",
      "|  4|winforms|\n",
      "+---+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfTagsOfTag.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id = 1, tag=data\n",
      "id = 4, tag=c#\n",
      "id = 4, tag=winforms\n",
      "id = 4, tag=type-conversion\n",
      "id = 4, tag=decimal\n"
     ]
    }
   ],
   "source": [
    "dfTagsOfTag.take(5).foreach(t => println(s\"id = ${t.id}, tag=${t.tag}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame row to Scala case class using map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class Question\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case class Question(owner_userid: Int, tag: String, creationDate: java.sql.Timestamp, score: Int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### create a function which will parse each element in the row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "toQuestion: (row: org.apache.spark.sql.Row)Question\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def toQuestion(row: org.apache.spark.sql.Row): Question = {\n",
    "    // to normalize our owner_userid data\n",
    "    val IntOf: String => Option[Int] = _ match {\n",
    "      case s if s == \"NA\" => None\n",
    "      case s => Some(s.toInt)\n",
    "    }\n",
    "\n",
    "    import java.time._\n",
    "    val DateOf: String => java.sql.Timestamp = _ match {\n",
    "      case s => java.sql.Timestamp.valueOf(ZonedDateTime.parse(s).toLocalDateTime)\n",
    "    }\n",
    "\n",
    "    Question (\n",
    "      owner_userid = IntOf(row.getString(0)).getOrElse(-1),\n",
    "      tag = row.getString(1),\n",
    "      creationDate = DateOf(row.getString(2)),\n",
    "      score = row.getString(3).toInt\n",
    "    )\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now let's convert each row into a Question case class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import spark.implicits._\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dfOfQuestion: org.apache.spark.sql.Dataset[Question] = [owner_userid: int, tag: string ... 2 more fields]\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dfOfQuestion: Dataset[Question] = dfQuestions.map(row => toQuestion(row))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create DataFrame from collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "seqTags: Seq[(Int, String)] = List((1,so_java), (1,so_jsp), (2,so_golang), (3,so_scala), (3,so_akka))\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val seqTags = Seq(\n",
    "  1 -> \"so_java\",\n",
    "  1 -> \"so_jsp\",\n",
    "  2 -> \"so_golang\",\n",
    "  3 -> \"so_scala\",\n",
    "  3 -> \"so_akka\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import spark.implicits._\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dfMoreTags: org.apache.spark.sql.DataFrame = [id: int, tag: string]\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dfMoreTags = seqTags.toDF(\"id\", \"tag\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+\n",
      "| id|      tag|\n",
      "+---+---------+\n",
      "|  1|  so_java|\n",
      "|  1|   so_jsp|\n",
      "|  2|so_golang|\n",
      "|  3| so_scala|\n",
      "|  3|  so_akka|\n",
      "+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfMoreTags.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "| id| tag|\n",
      "+---+----+\n",
      "|  1|data|\n",
      "|  4|  c#|\n",
      "+---+----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfTags.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "| id|    tag|\n",
      "+---+-------+\n",
      "|  1|so_java|\n",
      "|  1| so_jsp|\n",
      "+---+-------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfMoreTags.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dfUnionOfTags: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: int, tag: string]\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dfUnionOfTags = dfTags.union(dfMoreTags).filter(\"id in (1,3)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "| id|     tag|\n",
      "+---+--------+\n",
      "|  1|    data|\n",
      "|  1| so_java|\n",
      "|  1|  so_jsp|\n",
      "|  3|so_scala|\n",
      "|  3| so_akka|\n",
      "+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfUnionOfTags.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame Intersection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dfIntersectionTags: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: int, tag: string]\n"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dfIntersectionTags = dfMoreTags.intersect(dfUnionOfTags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "| id|     tag|\n",
      "+---+--------+\n",
      "|  3|so_scala|\n",
      "|  3| so_akka|\n",
      "|  1| so_java|\n",
      "|  1|  so_jsp|\n",
      "+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfIntersectionTags.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Append column to DataFrame using withColumn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dfSplitColumn: org.apache.spark.sql.DataFrame = [id: int, tag: string ... 2 more fields]\n"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dfSplitColumn = dfMoreTags.withColumn(\"tmp\", split($\"tag\", \"_\"))\n",
    "                              .select($\"id\",$\"tag\",$\"tmp\".getItem(0).as(\"so_prefix\"),$\"tmp\".getItem(1).as(\"so_tag\"))\n",
    "                              .drop(\"tmp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+---------+------+\n",
      "| id|      tag|so_prefix|so_tag|\n",
      "+---+---------+---------+------+\n",
      "|  1|  so_java|       so|  java|\n",
      "|  1|   so_jsp|       so|   jsp|\n",
      "|  2|so_golang|       so|golang|\n",
      "|  3| so_scala|       so| scala|\n",
      "|  3|  so_akka|       so|  akka|\n",
      "+---+---------+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfSplitColumn.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Closing Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
